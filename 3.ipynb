{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score , hamming_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    def __init__(self):\n",
    "        self.model = DecisionTreeClassifier()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "class MyLogisticClassifier:\n",
    "    def __init__(self, num_classes=9, learning_rate=0.005, num_epochs=5000):\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, y_pred, y_true):\n",
    "        m = y_pred.shape[0]\n",
    "        return -np.sum(np.log(y_pred) * y_true) / m\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        m, n = X_train.shape\n",
    "        self.weights = np.zeros((n, self.num_classes))\n",
    "        self.bias = np.zeros((1, self.num_classes))\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            z = np.dot(X_train, self.weights) + self.bias\n",
    "            y_pred = self.softmax(z)\n",
    "\n",
    "            y_train_encoded = np.eye(self.num_classes)[y_train]  # One-hot encoding\n",
    "            loss = self.cross_entropy_loss(y_pred, y_train_encoded)\n",
    "\n",
    "            dz = y_pred - y_train_encoded\n",
    "            dw = np.dot(X_train.T, dz) / m\n",
    "            db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self.softmax(z)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n",
    "class MyMLPClassifier(object):\n",
    "    def __init__(self, learning_rate=0.001,epochs = 5000, inputLayerSize: int = 12, hiddenLayerSize = [21] , outputLayerSize: int = 9):\n",
    "        self.inputLayerSize = inputLayerSize\n",
    "        self.outputLayerSize = outputLayerSize\n",
    "        self.hiddenLayerSize = hiddenLayerSize\n",
    "        self.activation = self.sigmoid\n",
    "        self.activationPrime = self.sigmoidPrime\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_layers = len(hiddenLayerSize)+1\n",
    "        self.weights = []\n",
    "        self.weights.append(np.random.randn(self.inputLayerSize, self.hiddenLayerSize[0]))\n",
    "        for i in range(1,self.num_layers-1):\n",
    "            self.weights.append(np.random.randn(self.hiddenLayerSize[i-1], self.hiddenLayerSize[i]))\n",
    "        self.weights.append(np.random.randn(self.hiddenLayerSize[-1], self.outputLayerSize))\n",
    "        self.optimizer = 'SGD'\n",
    "        self.max_iterations = epochs\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z = [None] * (len(self.weights))\n",
    "        self.a = [None] * (len(self.weights)-1)\n",
    "        \n",
    "        self.z[0] = np.dot(X, self.weights[0])\n",
    "        for i in range(0,len(self.a)):\n",
    "            self.a[i] = self.activation(self.z[i])\n",
    "            self.z[i+1] = np.dot(self.a[i],self.weights[i+1])\n",
    "        yHat = self.softmax(self.z[-1])\n",
    "        return yHat\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)  # ReLU activation\n",
    "\n",
    "    def tanh(self, z):\n",
    "        return np.tanh(z)  # Tanh activation\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoidPrime(self, z):\n",
    "        return np.exp(-z) / ((1 + np.exp(-z))**2)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z))  # Subtracting max(z) for numerical stability\n",
    "        return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def costFunction(self, X, y):\n",
    "        self.yHat = self.forward(X)\n",
    "        J = -np.sum(y * np.log(self.yHat)) / len(X)\n",
    "        return J\n",
    "\n",
    "    def costFunctionPrime(self, X, y):\n",
    "        self.yHat = self.forward(X)\n",
    "        deltas = [None] * (len(self.weights))\n",
    "        deltas[-1] = self.yHat - y\n",
    "        dJdWs = [None] * (len(self.weights))\n",
    "        for i in range(len(deltas)-2,-1,-1):\n",
    "            dJdWs[i+1] = np.dot(self.a[i].T, deltas[i+1])\n",
    "            deltas[i] = np.dot(deltas[i+1],self.weights[i+1].T)*self.activationPrime(self.z[i])\n",
    "        dJdWs[0] = np.dot(X.T, deltas[0])\n",
    "        return dJdWs\n",
    "\n",
    "    def reluPrime(self, z):\n",
    "        return np.where(z > 0, 1, 0)  # Derivative of ReLU\n",
    "\n",
    "    def tanhPrime(self, z):\n",
    "        return 1 - np.tanh(z)**2  # Derivative of Tanh\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        dJdWs = self.costFunctionPrime(X, y)\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * dJdWs[i]\n",
    "    def fit(self, X, y, batch_size = 32, learning_rate=0.1, max_iterations=10000,wand = 0):\n",
    "        if(self.optimizer == 'SGD'):\n",
    "            return self.train_sgd(X=X, y=y,wand=wand)\n",
    "        elif(self.optimizer == 'Batch'):\n",
    "            return self.train_batch(X=X, y=y,wand=wand)\n",
    "        elif(self.optimizer == 'MiniBatch'):\n",
    "            return self.train_mini_batch(X=X, y=y,wand=wand)\n",
    "    def train_sgd(self, X, y, learning_rate=0.1, max_iterations=10000, wand = 0):\n",
    "        if(wand == 1):\n",
    "            wandb.init(project=\"Reporting loss on MLP Classifier for WineQT Dataset\")\n",
    "        for i in range(self.max_iterations):\n",
    "            idx = np.random.randint(len(X))\n",
    "            self.backward(X[idx:idx+1], y[idx:idx+1], self.learning_rate)\n",
    "            if i % 1 == 0:\n",
    "                loss = self.costFunction(X, y)\n",
    "                # print(\"Iteration %d: loss = %f\" % (i, loss))\n",
    "                if(wand == 1):\n",
    "                    wandb.log({\"Loss\": loss, \"Epoch\": i})\n",
    "        loss = self.costFunction(X, y)\n",
    "        return loss\n",
    "        print(\"Training completed.\")\n",
    "    def train_batch(self, X, y, learning_rate=0.0001, max_iterations=10000, wand = 0):\n",
    "        if(wand == 1):\n",
    "            wandb.init(project=\"Reporting loss on MLP Classifier for WineQT Dataset\")\n",
    "        for i in range(self.max_iterations):\n",
    "            self.backward(X, y, self.learning_rate)\n",
    "            if i % 1 == 0:\n",
    "                loss = self.costFunction(X, y)\n",
    "                # print(\"Iteration %d: loss = %f\" % (i, loss))\n",
    "                if(wand == 1):\n",
    "                    wandb.log({\"Loss\": loss, \"Epoch\": i})\n",
    "        loss = self.costFunction(X, y)\n",
    "        return loss\n",
    "        print(\"Training completed.\")\n",
    "    def train_mini_batch(self, X, y, batch_size=32, learning_rate=0.01, max_iterations=5000, wand = 0):\n",
    "        if(wand == 1):\n",
    "            wandb.init(project=\"Reporting loss on MLP Classifier for WineQT Dataset\")\n",
    "        for i in range(self.max_iterations):\n",
    "            indices = np.arange(len(X))\n",
    "            np.random.shuffle(indices)\n",
    "            for j in range(0, len(X), batch_size):\n",
    "                batch_indices = indices[j:j+batch_size]\n",
    "                self.backward(X[batch_indices], y[batch_indices], self.learning_rate)\n",
    "            if i % 1 == 0:\n",
    "                loss = self.costFunction(X, y)\n",
    "                # print(\"Iteration %d: loss = %f\" % (i, loss))\n",
    "                if(wand == 1):\n",
    "                    wandb.log({\"Loss\": loss, \"Epoch\": i})\n",
    "        loss = self.costFunction(X, y)\n",
    "        return loss\n",
    "        print(\"Training completed.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        yHat = self.forward(X)\n",
    "        # return yHat\n",
    "        binary_predictions = np.zeros_like(yHat)\n",
    "        binary_predictions[np.arange(len(yHat)), yHat.argmax(axis=1)] = 1\n",
    "        return binary_predictions\n",
    "        # return np.argmax(yHat, axis=1)\n",
    "    def set_learning_rate(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def set_activation_function(self, activation_function):\n",
    "        if(activation_function == 'sigmoid'):\n",
    "            self.activation = self.sigmoid\n",
    "            self.activationPrime = self.sigmoidPrime\n",
    "        if(activation_function == 'relu'):\n",
    "            self.activation = self.relu\n",
    "            self.activationPrime = self.reluPrime\n",
    "        if(activation_function == 'tanh'):\n",
    "            self.activation = self.tanh\n",
    "            self.activationPrime = self.tanhPrime\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def set_hidden_layers(self, hidden_layer_sizes):\n",
    "        self.hiddenLayerSize = hidden_layer_sizes\n",
    "        self.num_layers = len(self.hiddenLayerSize)+1\n",
    "        self.weights = []\n",
    "        self.weights.append(np.random.randn(self.inputLayerSize, self.hiddenLayerSize[0]))\n",
    "        for i in range(1,self.num_layers-1):\n",
    "            self.weights.append(np.random.randn(self.hiddenLayerSize[i-1], self.hiddenLayerSize[i]))\n",
    "        self.weights.append(np.random.randn(self.hiddenLayerSize[-1], self.outputLayerSize))\n",
    "        # self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        # self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "        # self.weights, self.biases = self.initialize_weights()\n",
    "    def set_epochs(self, epcohs):\n",
    "        self.max_iterations = epochs\n",
    "        print(self.max_iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeRegressor:\n",
    "    def __init__(self):\n",
    "        self.model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "class MyLinearRegressor:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "        # print(X.T)\n",
    "        # Closed-form solution for linear regression\n",
    "        X_transpose = X.T\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        # print(y.shape)\n",
    "        # print(X.shape)\n",
    "        # print(X_transpose)\n",
    "        self.weights = np.dot(np.linalg.inv(np.dot(X_transpose, X)), np.dot(X_transpose, y))\n",
    "        # print(self.weights)\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        return linear_model\n",
    "    \n",
    "class MyMLPRegressor(object):\n",
    "    def __init__(self, learning_rate=0.001,epochs = 5000, inputLayerSize: int = 13, hiddenLayerSize = [13,13,13,13,13,13,13,13,13] , outputLayerSize: int = 1):\n",
    "        self.inputLayerSize = inputLayerSize\n",
    "        self.outputLayerSize = outputLayerSize\n",
    "        self.hiddenLayerSize = hiddenLayerSize\n",
    "        self.activation = self.sigmoid\n",
    "        self.activationPrime = self.sigmoidPrime\n",
    "        self.learning_rate = learning_rate\n",
    "        # Weights (parameters)\n",
    "        self.num_layers = len(hiddenLayerSize)+1\n",
    "        self.weights = []\n",
    "        self.weights.append(np.random.randn(self.inputLayerSize, self.hiddenLayerSize[0]))\n",
    "        for i in range(1,self.num_layers-1):\n",
    "            self.weights.append(np.random.randn(self.hiddenLayerSize[i-1], self.hiddenLayerSize[i]))\n",
    "        self.weights.append(np.random.randn(self.hiddenLayerSize[-1], self.outputLayerSize))\n",
    "        self.optimizer = 'MiniBatch'\n",
    "        self.max_iterations = epochs\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z = [None] * (len(self.weights))\n",
    "        self.a = [None] * (len(self.weights) - 1)\n",
    "\n",
    "        self.z[0] = np.dot(X, self.weights[0])\n",
    "        for i in range(0, len(self.a)):\n",
    "            self.a[i] = self.activation(self.z[i])\n",
    "            self.z[i + 1] = np.dot(self.a[i], self.weights[i + 1])\n",
    "        yHat = self.z[-1]  # Linear activation for regression\n",
    "        return yHat\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)  # ReLU activation\n",
    "\n",
    "    def tanh(self, z):\n",
    "        return np.tanh(z)  # Tanh activation\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoidPrime(self, z):\n",
    "        return np.exp(-z) / ((1 + np.exp(-z))**2)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z))  # Subtracting max(z) for numerical stability\n",
    "        return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def costFunction(self, X, y):\n",
    "        self.yHat = self.forward(X)\n",
    "        J = np.sum((y - self.yHat) ** 2) / (2 * len(X))  # Mean Squared Error\n",
    "        return J\n",
    "\n",
    "    def costFunctionPrime(self, X, y):\n",
    "        self.yHat = self.forward(X)\n",
    "        deltas = [None] * (len(self.weights))\n",
    "        deltas[-1] = -(y - self.yHat)\n",
    "        dJdWs = [None] * (len(self.weights))\n",
    "        for i in range(len(deltas) - 2, -1, -1):\n",
    "            dJdWs[i + 1] = np.dot(self.a[i].T, deltas[i + 1])\n",
    "            deltas[i] = np.dot(deltas[i + 1], self.weights[i + 1].T) * self.activationPrime(self.z[i])\n",
    "        dJdWs[0] = np.dot(X.T, deltas[0])\n",
    "        return dJdWs\n",
    "\n",
    "    def reluPrime(self, z):\n",
    "        return np.where(z > 0, 1, 0)  # Derivative of ReLU\n",
    "\n",
    "    def tanhPrime(self, z):\n",
    "        return 1 - np.tanh(z)**2  # Derivative of Tanh\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        dJdWs = self.costFunctionPrime(X, y)\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * dJdWs[i]\n",
    "        # self.W1 -= learning_rate * dJdW1\n",
    "        # self.W2 -= learning_rate * dJdW2\n",
    "    def fit(self, X, y, batch_size = 32, learning_rate=0.1, max_iterations=10000,wand = 0):\n",
    "        if(self.optimizer == 'SGD'):\n",
    "            return self.train_sgd(X=X, y=y,wand=wand)\n",
    "        elif(self.optimizer == 'Batch'):\n",
    "            return self.train_batch(X=X, y=y,wand=wand)\n",
    "        elif(self.optimizer == 'MiniBatch'):\n",
    "            return self.train_mini_batch(X=X, y=y,wand=wand)\n",
    "    def train_sgd(self, X, y, learning_rate=0.1, max_iterations=10000, wand = 0):\n",
    "        if(wand == 1):\n",
    "            wandb.init(project=\"Reporting loss on Housing Dataset in MLP Regression\")\n",
    "        for i in range(self.max_iterations):\n",
    "            idx = np.random.randint(len(X))\n",
    "            self.backward(X[idx:idx+1], y[idx:idx+1], self.learning_rate)\n",
    "            if i % 100 == 0:\n",
    "                loss = self.costFunction(X, y)\n",
    "                print(\"Iteration %d: loss = %f\" % (i, loss))\n",
    "                if(wand == 1):\n",
    "                    wandb.log({\"Loss\": loss, \"Epoch\": i})\n",
    "        loss = self.costFunction(X, y)\n",
    "        return loss\n",
    "        print(\"Training completed.\")\n",
    "    def train_batch(self, X, y, learning_rate=0.0001, max_iterations=10000, wand = 0):\n",
    "        if(wand == 1):\n",
    "            wandb.init(project=\"Reporting loss on Housing Dataset in MLP Regression\")\n",
    "        for i in range(self.max_iterations):\n",
    "            self.backward(X, y, self.learning_rate)\n",
    "            if i % 1 == 0:\n",
    "                loss = self.costFunction(X, y)\n",
    "                # print(\"Iteration %d: loss = %f\" % (i, loss))\n",
    "                if(wand == 1):\n",
    "                    wandb.log({\"Loss\": loss, \"Epoch\": i})\n",
    "        loss = self.costFunction(X, y)\n",
    "        return loss\n",
    "        print(\"Training completed.\")\n",
    "    def train_mini_batch(self, X, y, batch_size=32, learning_rate=0.01, max_iterations=5000, wand = 0):\n",
    "        if(wand == 1):\n",
    "            wandb.init(project=\"Reporting loss on Housing Dataset in MLP Regression\")\n",
    "        for i in range(self.max_iterations):\n",
    "            # Randomly shuffle the data and split into mini-batches\n",
    "            indices = np.arange(len(X))\n",
    "            np.random.shuffle(indices)\n",
    "            for j in range(0, len(X), batch_size):\n",
    "                batch_indices = indices[j:j+batch_size]\n",
    "                self.backward(X[batch_indices], y[batch_indices], self.learning_rate)\n",
    "            if i % 1 == 0:\n",
    "                loss = self.costFunction(X, y)\n",
    "                # print(\"Iteration %d: loss = %f\" % (i, loss))\n",
    "                if(wand == 1):\n",
    "                    wandb.log({\"Loss\": loss, \"Epoch\": i})\n",
    "        loss = self.costFunction(X, y)\n",
    "        return loss\n",
    "        print(\"Training completed.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        yHat = self.forward(X)\n",
    "        return yHat\n",
    "    def set_learning_rate(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def set_activation_function(self, activation_function):\n",
    "        if(activation_function == 'sigmoid'):\n",
    "            self.activation = self.sigmoid\n",
    "            self.activationPrime = self.sigmoidPrime\n",
    "        if(activation_function == 'relu'):\n",
    "            self.activation = self.relu\n",
    "            self.activationPrime = self.reluPrime\n",
    "        if(activation_function == 'tanh'):\n",
    "            self.activation = self.tanh\n",
    "            self.activationPrime = self.tanhPrime\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def set_hidden_layers(self, hidden_layer_sizes):\n",
    "        self.hiddenLayerSize = hidden_layer_sizes\n",
    "        self.num_layers = len(self.hiddenLayerSize)+1\n",
    "        self.weights = []\n",
    "        self.weights.append(np.random.randn(self.inputLayerSize, self.hiddenLayerSize[0]))\n",
    "        for i in range(1,self.num_layers-1):\n",
    "            self.weights.append(np.random.randn(self.hiddenLayerSize[i-1], self.hiddenLayerSize[i]))\n",
    "        self.weights.append(np.random.randn(self.hiddenLayerSize[-1], self.outputLayerSize))\n",
    "        # self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        # self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "        # self.weights, self.biases = self.initialize_weights()\n",
    "    def set_epochs(self, epcohs):\n",
    "        self.max_iterations = epochs\n",
    "        print(self.max_iterations)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('HousingData.csv')\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "data = data.fillna(data.mean())\n",
    "X = data.drop(columns=['MEDV'])\n",
    "y = data['MEDV']\n",
    "# X = data.iloc[1:, :-1].reset_index(drop=True)\n",
    "# y = data.iloc[1:, -1].reset_index(drop=True)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.18056442e-04 3.47368421e-01 3.65623822e-02 ... 6.17021277e-01\n",
      "  9.91833629e-01 1.43487859e-01]\n",
      " [2.97300779e-04 2.63157895e-01 1.37580098e-01 ... 6.80851064e-01\n",
      "  9.84123764e-01 1.59492274e-01]\n",
      " [1.74592552e-03 2.31578947e-01 1.75273276e-01 ... 6.91489362e-01\n",
      "  9.43723054e-01 2.14955850e-01]\n",
      " ...\n",
      " [6.68786251e-05 8.42105263e-01 3.01545420e-02 ... 4.68085106e-01\n",
      "  9.84884606e-01 1.17549669e-01]\n",
      " [1.25342233e-01 0.00000000e+00 6.36637769e-01 ... 8.08510638e-01\n",
      "  2.72001014e-01 5.94370861e-01]\n",
      " [2.46945108e-03 0.00000000e+00 2.77044855e-01 ... 8.82978723e-01\n",
      "  1.72964748e-01 2.45584989e-01]]\n",
      "[36.1 22.9 24.5 25.  50.  34.9 31.7 24.1 22.1 14.1 42.8 19.3 32.2 26.4\n",
      " 21.8 21.7  8.3 46.7 43.1 31.5 10.5 16.7 20.  33.3 17.8 50.  20.5 23.2\n",
      " 13.1 19.6 22.8 28.7 30.7 22.9 21.9 23.9 32.7 24.3 21.5 24.6  8.5 26.4\n",
      " 23.1 15.   8.8 19.3 23.9 24.7 19.8 23.8 13.3 29.  27.1 34.6 13.3 15.6\n",
      " 12.5 14.6 11.  24.8 17.3  8.1 21.4 15.6 23.3 32.  38.7 30.1 20.5 32.5\n",
      " 42.3 24.3 20.6 22.  18.2 15.   6.3 20.1 21.4 28.4 30.1 20.8 23.  14.3\n",
      " 11.7 37.3 17.1 10.4 23.  22.7 20.3 21.7 50.   8.4 18.8 37.2 16.1 16.5\n",
      " 22.2 20.6 13.5 48.3 23.8 22.7 17.4 30.3 36.  41.7 18.3 22.  18.6 44.8\n",
      " 11.9 18.7 16.2 22.   7.2 20.4 13.8 13.  18.4 23.1 21.2 23.1 23.5 50.\n",
      " 26.6 22.2 50.   8.3 23.3 21.7 18.9 18.4 17.4 13.4 12.1 26.6 21.7 28.4\n",
      " 20.5 22.  13.9 11.3 29.9 26.6 10.5 23.2 24.4 46.  21.9  7.5 36.2 44.\n",
      " 17.8 27.5 37.6 14.1 28.1 10.2 19.1 43.8 27.9 25.  16.  16.6 13.2 50.\n",
      " 22.2 32.9 15.2 14.8 13.8 24.3 33.8 22.3 50.   9.5 13.3 22.2 18.1 18.\n",
      " 25.  16.5 23.  20.1 33.  24.8 18.2 13.1 34.9 10.2 19.9 27.9 23.3 35.1\n",
      " 12.8 22.  18.5 25.1 22.5 22.4 28.6 19.5 24.8 24.5 21.4 33.1 22.9 20.7\n",
      " 24.1 50.  24.7 28.7  7.2 37.  20.3 30.1 19.5 23.4 11.5 21.6 14.9 15.2\n",
      " 19.4  8.4 28.  22.6 13.5 14.5 31.  10.9 21.9 22.  19.  21.4 25.  17.5\n",
      " 36.5 20.1 20.4 16.2 23.6  7.4 35.2 50.  19.3 21.2 15.6 33.4 19.1 21.\n",
      " 23.7 18.9 16.8 19.7 17.7 22.6 11.8 34.9 20.6 20.2 32.  22.3 23.3 14.4\n",
      " 31.2 24.  29.6 19.6 21.6 20.  27.  33.2 15.4 30.5  7.2 23.9 16.3 23.9\n",
      " 50.  22.8 15.4 19.2 19.6 22.6 33.2 50.  22.2 14.9 19.8 23.7 19.  20.3\n",
      " 11.9 13.6 29.8 21.7 19.5 21.1 24.5 13.4 18.6]\n"
     ]
    }
   ],
   "source": [
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_std = std_scaler.fit_transform(X_train)\n",
    "X_val_std = std_scaler.transform(X_val)\n",
    "X_test_std = std_scaler.transform(X_test)\n",
    "\n",
    "X_train_norm = minmax_scaler.fit_transform(X_train_std)\n",
    "X_val_norm = minmax_scaler.transform(X_val_std)\n",
    "X_test_norm = minmax_scaler.transform(X_test_std)\n",
    "X_train = X_train_norm\n",
    "X_val = X_val_norm\n",
    "X_test = X_test_norm\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 11.196675712448528\n"
     ]
    }
   ],
   "source": [
    "def bagging_ensemble_regression(base_estimator, n_estimators, sample_fraction,with_replacement, voting):\n",
    "    ensemble_models = []\n",
    "    \n",
    "    sample_size = int(sample_fraction * len(X_train))\n",
    "    # np.random.seed(42)\n",
    "    for _ in range(n_estimators):\n",
    "        sample_indices = np.random.choice(len(X_train), size=sample_size, replace=with_replacement)\n",
    "        sample_X = X_train[sample_indices]\n",
    "        sample_y = y_train[sample_indices]\n",
    "        sample_y = np.array(sample_y)\n",
    "        if base_estimator == \"DecisionTree\":\n",
    "            base_model = MyDecisionTreeRegressor()\n",
    "            base_model.fit(sample_X, sample_y)\n",
    "        elif base_estimator == \"Linear\":\n",
    "            base_model = MyLinearRegressor()\n",
    "            base_model.fit(sample_X, sample_y)\n",
    "        elif base_estimator == \"MLP\":\n",
    "            base_model = MyMLPRegressor()\n",
    "            \n",
    "            y_train_array = np.array([[x] for x in sample_y])\n",
    "            base_model.fit(sample_X, y_train_array)\n",
    "            \n",
    "\n",
    "        # base_model.fit(sample_X, sample_y)\n",
    "        ensemble_models.append(base_model)\n",
    "\n",
    "    def ensemble_predict(X):\n",
    "        predictions = [model.predict(X) for model in ensemble_models]\n",
    "\n",
    "        if voting == \"hard\":\n",
    "            return np.round(np.mean(predictions, axis=0))\n",
    "        elif voting == \"soft\":\n",
    "            confidences = [1 / (mean_squared_error(y_val, model.predict(X_val)) + 1e-10) for model in ensemble_models]  \n",
    "            weighted_predictions = [prediction * confidence for prediction, confidence in zip(predictions, confidences)]\n",
    "            return np.sum(weighted_predictions, axis=0) / np.sum(confidences)\n",
    "\n",
    "    return ensemble_predict\n",
    "\n",
    "bagged_regressor_decision_tree_soft = bagging_ensemble_regression(base_estimator=\"DecisionTree\", n_estimators=10, sample_fraction=0.75, with_replacement=True, voting=\"soft\")\n",
    "y_pred = bagged_regressor_decision_tree_soft(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 37.896960784313734\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.15\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 57.476078431372564\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.15\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 25.440294117647063\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.15\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 34.552549019607845\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.15\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 37.91137254901961\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.25\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 24.570196078431376\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.25\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 28.95205882352941\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.25\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 22.14666666666667\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.25\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 28.585000000000004\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.5\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 34.458431372549015\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.5\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 24.281666666666666\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.5\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 34.54470588235294\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.5\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 31.96166666666667\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.75\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 9.801568627450981\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.75\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 26.38941176470588\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.75\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 21.397647058823534\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.75\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 26.978039215686266\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 1\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 27.34470588235294\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 1\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 11.666960784313725\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 1\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 11.6878431372549\n",
      "n_estimators: 1\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 1\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 42.128362584258035\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.15\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 47.39764705882352\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.15\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 47.988397023159166\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.15\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 56.10941176470588\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.15\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 45.01852045317267\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.25\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 40.87607843137255\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.25\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 36.557000300578764\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.25\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 40.962352941176476\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.25\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 41.10569786913363\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.5\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 38.29372549019608\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.5\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 36.96687397464687\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.5\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 42.75450980392157\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.5\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 40.21547910296138\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.75\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 36.40156862745098\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.75\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 36.79098861355782\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.75\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 35.20549019607843\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.75\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 35.7518940776401\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 1\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 37.31921568627451\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 1\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 35.808138063353695\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 1\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 35.58588235294118\n",
      "n_estimators: 1\n",
      "base_estimator: Linear\n",
      "sample_fraction: 1\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 25.871629908711995\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.15\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 31.156470588235294\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.15\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 23.636445204255626\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.15\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 22.99764705882353\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.15\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 14.289450109790254\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.25\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 18.67411764705882\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.25\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 26.619983268845914\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.25\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 25.223137254901967\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.25\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 23.011296353478013\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.5\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 17.434901960784313\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.5\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 24.638036880215907\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.5\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 19.505490196078433\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.5\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 12.488626495000881\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.75\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 32.6407843137255\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.75\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 16.10787557381802\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.75\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 18.934901960784316\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.75\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 14.22392684249949\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 1\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 19.919215686274512\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 1\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 12.386359164020215\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 1\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 11.640784313725492\n",
      "n_estimators: 1\n",
      "base_estimator: MLP\n",
      "sample_fraction: 1\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 11.46973468237132\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.15\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 22.462352941176473\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.15\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 16.7197833361324\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.15\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 22.132941176470588\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.15\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 11.692094799924233\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.25\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 13.24078431372549\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.25\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 16.996276462241433\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.25\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 18.63686274509804\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.25\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 11.46144922388375\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.5\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 12.31529411764706\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.5\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 10.10990896574024\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.5\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 15.485882352941175\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.5\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 8.709035616461907\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.75\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 11.115294117647059\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.75\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 7.405519376928269\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.75\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 15.478039215686275\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 0.75\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 9.2755958973825\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 1\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 8.383921568627452\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 1\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 11.666960784313728\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 1\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 11.6878431372549\n",
      "n_estimators: 5\n",
      "base_estimator: DecisionTree\n",
      "sample_fraction: 1\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 41.45519937161681\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.15\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 43.98980392156864\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.15\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 37.933312766153925\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.15\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 35.90941176470587\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.15\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 34.91386196989075\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.25\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 37.18\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.25\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 35.584928067535\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.25\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 37.19372549019607\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.25\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 35.28757854976465\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.5\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 35.927058823529414\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.5\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 34.708264504530284\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.5\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 37.321176470588235\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.5\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 36.21533089411364\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.75\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 35.58980392156863\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.75\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 35.25130791382273\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.75\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 34.94274509803921\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 0.75\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 36.067984072552505\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 1\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 37.68\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 1\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 35.808138063353695\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 1\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 35.58588235294118\n",
      "n_estimators: 5\n",
      "base_estimator: Linear\n",
      "sample_fraction: 1\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 19.722692534188642\n",
      "n_estimators: 5\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.15\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 18.029019607843136\n",
      "n_estimators: 5\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.15\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 17.44295797809898\n",
      "n_estimators: 5\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.15\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 16.911372549019607\n",
      "n_estimators: 5\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.15\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 12.734594377060873\n",
      "n_estimators: 5\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.25\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 15.31529411764706\n",
      "n_estimators: 5\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.25\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 13.272924854494766\n",
      "n_estimators: 5\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.25\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 12.38980392156863\n",
      "n_estimators: 5\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.25\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 17.905738498873774\n",
      "n_estimators: 5\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.5\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 13.04078431372549\n",
      "n_estimators: 5\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.5\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 11.565715974044153\n",
      "n_estimators: 5\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.5\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 13.809411764705883\n",
      "n_estimators: 5\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.5\n",
      "bootstrap: False\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 13.999338041648423\n",
      "n_estimators: 5\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.75\n",
      "bootstrap: True\n",
      "voting: soft\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 11.19764705882353\n",
      "n_estimators: 5\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.75\n",
      "bootstrap: True\n",
      "voting: hard\n",
      "-----------------------------------------------------\n",
      "Mean Squared Error: 9.001066031770861\n",
      "n_estimators: 5\n",
      "base_estimator: MLP\n",
      "sample_fraction: 0.75\n",
      "bootstrap: False\n",
      "voting: soft\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntulsa/Sem5/SMAI/Assignment 4/3.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m bootstrap:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m voting:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         bagged_regressor \u001b[39m=\u001b[39m bagging_ensemble_regression(base_estimator\u001b[39m=\u001b[39;49mj, n_estimators\u001b[39m=\u001b[39;49mi, sample_fraction\u001b[39m=\u001b[39;49mk, with_replacement\u001b[39m=\u001b[39;49ml, voting\u001b[39m=\u001b[39;49mm)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         y_pred \u001b[39m=\u001b[39m bagged_regressor(X_test)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         mse \u001b[39m=\u001b[39m mean_squared_error(y_test, y_pred)\n",
      "\u001b[1;32m/home/ubuntulsa/Sem5/SMAI/Assignment 4/3.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     base_model \u001b[39m=\u001b[39m MyMLPRegressor()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     y_train_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m sample_y])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     base_model\u001b[39m.\u001b[39;49mfit(sample_X, y_train_array)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# base_model.fit(sample_X, sample_y)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m ensemble_models\u001b[39m.\u001b[39mappend(base_model)\n",
      "\u001b[1;32m/home/ubuntulsa/Sem5/SMAI/Assignment 4/3.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_batch(X\u001b[39m=\u001b[39mX, y\u001b[39m=\u001b[39my,wand\u001b[39m=\u001b[39mwand)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39melif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMiniBatch\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_mini_batch(X\u001b[39m=\u001b[39;49mX, y\u001b[39m=\u001b[39;49my,wand\u001b[39m=\u001b[39;49mwand)\n",
      "\u001b[1;32m/home/ubuntulsa/Sem5/SMAI/Assignment 4/3.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(X), batch_size):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m     batch_indices \u001b[39m=\u001b[39m indices[j:j\u001b[39m+\u001b[39mbatch_size]\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackward(X[batch_indices], y[batch_indices], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_rate)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcostFunction(X, y)\n",
      "\u001b[1;32m/home/ubuntulsa/Sem5/SMAI/Assignment 4/3.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, X, y, learning_rate):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     dJdWs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcostFunctionPrime(X, y)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights)):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights[i] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m dJdWs[i]\n",
      "\u001b[1;32m/home/ubuntulsa/Sem5/SMAI/Assignment 4/3.ipynb Cell 8\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(deltas) \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     dJdWs[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma[i]\u001b[39m.\u001b[39mT, deltas[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     deltas[i] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(deltas[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights[i \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mT) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivationPrime(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz[i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m dJdWs[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(X\u001b[39m.\u001b[39mT, deltas[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dJdWs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_estimators = [1,5,10]\n",
    "base_estimator = [\"DecisionTree\",\"Linear\",\"MLP\"]\n",
    "sample_fraction = [0.15,0.25,0.5,0.75,1]\n",
    "bootstrap = [True,False]\n",
    "voting = [\"soft\",\"hard\"]\n",
    "best_mse = 10000000\n",
    "best_params = []\n",
    "for i in n_estimators:\n",
    "    for j in base_estimator:\n",
    "        for k in sample_fraction:\n",
    "            for l in bootstrap:\n",
    "                for m in voting:\n",
    "                    bagged_regressor = bagging_ensemble_regression(base_estimator=j, n_estimators=i, sample_fraction=k, with_replacement=l, voting=m)\n",
    "                    y_pred = bagged_regressor(X_test)\n",
    "                    mse = mean_squared_error(y_test, y_pred)\n",
    "                    print(\"Mean Squared Error:\", mse)\n",
    "                    print(\"n_estimators:\",i)\n",
    "                    print(\"base_estimator:\",j)\n",
    "                    print(\"sample_fraction:\",k)\n",
    "                    print(\"bootstrap:\",l)\n",
    "                    print(\"voting:\",m)\n",
    "                    print(\"-----------------------------------------------------\")\n",
    "                    if(mse < best_mse):\n",
    "                        best_mse = mse\n",
    "                        best_params = [i,j,k,l,m]\n",
    "print(\"Best MSE:\",best_mse)\n",
    "print(\"Best Parameters:\",best_params)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('WineQT.csv', header=0)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2,random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')  \n",
    "\n",
    "train_data_scaled = scaler.fit_transform(imputer.fit_transform(train_data.drop(columns=['quality'])))\n",
    "train_labels = train_data['quality']\n",
    "\n",
    "test_data_scaled = scaler.transform(imputer.transform(test_data.drop(columns=['quality'])))\n",
    "\n",
    "train_data_scaled_normalized = minmax_scaler.fit_transform(train_data_scaled)\n",
    "test_data_scaled_normalized = minmax_scaler.transform(test_data_scaled)\n",
    "\n",
    "X_train = train_data_scaled\n",
    "y_train = train_labels\n",
    "X_test = test_data_scaled\n",
    "y_test = test_data['quality']\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "# y_train = pd.get_dummies(y_train).astype(int)\n",
    "# y_train = np.eye(9)[y_train]\n",
    "# y_train = np.array(y_train)\n",
    "# y_test = pd.get_dummies(y_test).astype(int)\n",
    "# print(X_train)\n",
    "# print(y_train)\n",
    "# y_test = np.eye(9)[y_test]\n",
    "# y_test = np.array(y_test)\n",
    "y_test = np.eye(9)[y_test]\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6157205240174672, 0.611353711790393, 0.6331877729257642, 0.5982532751091703, 0.62882096069869, 0.62882096069869, 0.5807860262008734, 0.5764192139737991, 0.6157205240174672, 0.5720524017467249]\n",
      "[0.        0.        0.        0.        0.        0.8962536 0.1037464\n",
      " 0.        0.       ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.69596542\n",
      " 0.20244957 0.10158501 0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.79971182 0.20028818 0.        ]\n",
      "6\n",
      "[0.        0.        0.        0.        0.        0.        0.1981268\n",
      " 0.8018732 0.       ]\n",
      "7\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.30115274\n",
      " 0.69884726 0.         0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.10086455 0.89913545 0.        ]\n",
      "7\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.79466859 0.20533141 0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.89553314\n",
      " 0.10446686 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.90489914 0.09510086 0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.09510086\n",
      " 0.90489914 0.         0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.50144092\n",
      " 0.49855908 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.29755043\n",
      " 0.70244957 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.80043228\n",
      " 0.19956772 0.         0.        ]\n",
      "5\n",
      "[0.        0.        0.        0.        0.        0.        0.6037464\n",
      " 0.3962536 0.       ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.20533141\n",
      " 0.29394813 0.50072046 0.        ]\n",
      "7\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.59582133 0.40417867 0.        ]\n",
      "6\n",
      "[0.        0.        0.        0.        0.        0.8962536 0.1037464\n",
      " 0.        0.       ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.39913545 0.60086455 0.        ]\n",
      "7\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.29106628\n",
      " 0.70893372 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.70821326 0.29178674 0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.68804035\n",
      " 0.31195965 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.09510086 0.90489914 0.        ]\n",
      "7\n",
      "[0.         0.         0.         0.         0.         0.29322767\n",
      " 0.70677233 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.79755043\n",
      " 0.20244957 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.70244957\n",
      " 0.29755043 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.40850144 0.59149856 0.        ]\n",
      "7\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.40417867 0.59582133 0.        ]\n",
      "7\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.20821326 0.79178674 0.        ]\n",
      "7\n",
      "[0.         0.         0.         0.         0.         0.80403458\n",
      " 0.19596542 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.59870317\n",
      " 0.40129683 0.         0.        ]\n",
      "5\n",
      "[0.        0.        0.        0.        0.        0.1037464 0.8962536\n",
      " 0.        0.       ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.09510086\n",
      " 0.90489914 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.19308357 0.80691643 0.        ]\n",
      "7\n",
      "[0.         0.         0.         0.         0.         0.90489914\n",
      " 0.09510086 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.69308357\n",
      " 0.30691643 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.30043228\n",
      " 0.49927954 0.20028818 0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.20821326\n",
      " 0.79178674 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.48991354 0.51008646 0.        ]\n",
      "7\n",
      "[0.         0.         0.         0.         0.         0.89913545\n",
      " 0.10086455 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.09510086\n",
      " 0.90489914 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.69884726 0.30115274 0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.80403458\n",
      " 0.19596542 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.40417867\n",
      " 0.59582133 0.         0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.90489914 0.09510086 0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.49567723 0.50432277 0.        ]\n",
      "7\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.        0.        0.        0.        0.        0.1037464 0.8962536\n",
      " 0.        0.       ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.70821326\n",
      " 0.29178674 0.         0.        ]\n",
      "5\n",
      "[0.        0.        0.        0.        0.        0.8962536 0.1037464\n",
      " 0.        0.       ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.09870317\n",
      " 0.90129683 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.09510086 0.\n",
      " 0.90489914 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.90489914\n",
      " 0.09510086 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.09510086\n",
      " 0.80115274 0.1037464  0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.60014409\n",
      " 0.39985591 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.69380403\n",
      " 0.30619597 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.29538905\n",
      " 0.70461095 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.50864553 0.49135447 0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.79466859\n",
      " 0.20533141 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.19092219 0.30619597\n",
      " 0.50288184 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.40850144\n",
      " 0.59149856 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.09510086 0.\n",
      " 0.80907781 0.09582133 0.        ]\n",
      "6\n",
      "[0.        0.        0.        0.        0.        0.8962536 0.1037464\n",
      " 0.        0.       ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.19092219 0.80907781\n",
      " 0.         0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.59942363\n",
      " 0.40057637 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.69596542\n",
      " 0.20244957 0.10158501 0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.79466859\n",
      " 0.20533141 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.29971182 0.70028818 0.        ]\n",
      "7\n",
      "[0.         0.         0.         0.         0.         0.30115274\n",
      " 0.6037464  0.09510086 0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.69668588\n",
      " 0.30331412 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.61383285 0.38616715 0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.59870317 0.40129683 0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.39697406 0.60302594 0.        ]\n",
      "7\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.89913545\n",
      " 0.10086455 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.69092219\n",
      " 0.30907781 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "7\n",
      "[0.        0.        0.        0.        0.        0.1037464 0.8962536\n",
      " 0.        0.       ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.29971182 0.70028818 0.        ]\n",
      "7\n",
      "[0.         0.         0.         0.         0.         0.79394813\n",
      " 0.20605187 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.        0.        0.        0.        0.        0.0943804 0.9056196\n",
      " 0.        0.       ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.69092219\n",
      " 0.10086455 0.20821326 0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.79178674 0.20821326 0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.49855908\n",
      " 0.         0.50144092 0.        ]\n",
      "7\n",
      "[0.         0.         0.         0.         0.         0.49567723\n",
      " 0.50432277 0.         0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "7\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.19884726\n",
      " 0.80115274 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.19596542\n",
      " 0.80403458 0.         0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "7\n",
      "[0.         0.         0.         0.         0.         0.79466859\n",
      " 0.20533141 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.80403458\n",
      " 0.19596542 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.39841499 0.60158501 0.        ]\n",
      "7\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.69596542\n",
      " 0.30403458 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.19884726 0.80115274 0.        ]\n",
      "7\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.30259366 0.69740634 0.        ]\n",
      "7\n",
      "[0.         0.         0.         0.         0.         0.89913545\n",
      " 0.10086455 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.09510086\n",
      " 0.80115274 0.1037464  0.        ]\n",
      "6\n",
      "[0.        0.        0.        0.        0.        0.5943804 0.4056196\n",
      " 0.        0.       ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.49927954 0.50072046 0.        ]\n",
      "7\n",
      "[0.         0.         0.         0.         0.         0.09510086\n",
      " 0.90489914 0.         0.        ]\n",
      "6\n",
      "[0.        0.        0.        0.        0.        0.1037464 0.8962536\n",
      " 0.        0.       ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.19740634\n",
      " 0.80259366 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.19740634\n",
      " 0.80259366 0.         0.        ]\n",
      "6\n",
      "[0.        0.        0.        0.        0.        0.8962536 0.1037464\n",
      " 0.        0.       ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.90489914 0.09510086 0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.09510086\n",
      " 0.90489914 0.         0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.49495677 0.50504323 0.        ]\n",
      "7\n",
      "[0.         0.         0.         0.         0.         0.59149856\n",
      " 0.40850144 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.49351585\n",
      " 0.50648415 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.90417867\n",
      " 0.09582133 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.90489914\n",
      " 0.09510086 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.79971182 0.20028818 0.        ]\n",
      "6\n",
      "[0.        0.        0.        0.        0.        0.1037464 0.8962536\n",
      " 0.        0.       ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.59798271\n",
      " 0.40201729 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.50216138\n",
      " 0.49783862 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.89913545\n",
      " 0.         0.10086455 0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.39193084\n",
      " 0.50648415 0.10158501 0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.89841499\n",
      " 0.10158501 0.         0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.70028818\n",
      " 0.29971182 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.89841499 0.10158501 0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.39481268 0.60518732 0.        ]\n",
      "7\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.19308357\n",
      " 0.80691643 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.29755043 0.70244957 0.        ]\n",
      "7\n",
      "[0.         0.         0.         0.         0.         0.50792507\n",
      " 0.49207493 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.09510086\n",
      " 0.90489914 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.39553314\n",
      " 0.60446686 0.         0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.59798271 0.40201729 0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.49423631 0.50576369 0.        ]\n",
      "7\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.70244957 0.29755043 0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.09510086 0.\n",
      " 0.90489914 0.         0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.09870317\n",
      " 0.90129683 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.69308357\n",
      " 0.20244957 0.10446686 0.        ]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.89841499 0.10158501 0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.09510086\n",
      " 0.90489914 0.         0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "7\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.60302594 0.39697406 0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.40129683\n",
      " 0.59870317 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.09510086 0.19668588\n",
      " 0.70821326 0.         0.        ]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.19668588\n",
      " 0.80331412 0.         0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.10158501\n",
      " 0.89841499 0.         0.        ]\n",
      "6\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "5\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "6\n",
      "[0.         0.         0.         0.         0.         0.89553314\n",
      " 0.10446686 0.         0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.09510086 0.59582133\n",
      " 0.10086455 0.20821326 0.        ]\n",
      "5\n",
      "[0.         0.         0.         0.         0.         0.29682997\n",
      " 0.70317003 0.         0.        ]\n",
      "6\n",
      "(229, 9)\n",
      "(229, 9)\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Accuracy: 0.6200873362445415\n"
     ]
    }
   ],
   "source": [
    "def bagging_ensemble_classification(base_classifier, n_estimators, sample_fraction, with_replacement, voting):\n",
    "    ensemble_models = []\n",
    "    \n",
    "    sample_size = int(sample_fraction * len(X_train))\n",
    "    # np.random.seed(42)\n",
    "    for _ in range(n_estimators):\n",
    "        sample_indices = np.random.choice(len(X_train), size=sample_size, replace=with_replacement)\n",
    "        sample_X = X_train[sample_indices]  # Assuming X_train is a DataFrame\n",
    "        sample_y = y_train[sample_indices]  # Assuming y_train is a Series\n",
    "\n",
    "        if base_classifier == \"DecisionTree\":\n",
    "            base_model = MyDecisionTreeClassifier()\n",
    "            sample_y = np.eye(9)[sample_y]\n",
    "            sample_y = np.array(sample_y)\n",
    "            base_model.fit(sample_X, sample_y)\n",
    "        elif base_classifier == \"Logistic\":\n",
    "            base_model = MyLogisticClassifier()\n",
    "            base_model.fit(sample_X, sample_y)\n",
    "        elif base_classifier == \"MLP\":\n",
    "            base_model = MyMLPClassifier()\n",
    "            sample_y = np.eye(9)[sample_y]\n",
    "            sample_y = np.array(sample_y)\n",
    "            base_model.fit(sample_X, sample_y)\n",
    "            \n",
    "        ensemble_models.append(base_model)\n",
    "\n",
    "    def ensemble_predict(X):\n",
    "        predictions = [model.predict(X) for model in ensemble_models]\n",
    "        predictions = np.array(predictions)\n",
    "        if(base_classifier == \"DecisionTree\"):\n",
    "            if voting == \"hard\":\n",
    "                array_2d = np.sum(predictions, axis=0)\n",
    "                for array in range(len(array_2d)):\n",
    "                    k = np.argmax(array_2d[array])\n",
    "                    array_2d[array] = np.zeros(9)\n",
    "                    array_2d[array][k] = 1\n",
    "                return array_2d\n",
    "\n",
    "            elif voting == \"soft\":\n",
    "                confidences = [accuracy_score(y_test, model.predict(X_test)) for model in ensemble_models]\n",
    "                confidences = confidences / sum(confidences)\n",
    "                print(confidences)\n",
    "                for i in range(len(confidences)):\n",
    "                    predictions[i] = predictions[i] * confidences[i]\n",
    "                array_2d = np.sum(predictions, axis=0)\n",
    "                for array in range(len(array_2d)):\n",
    "                    k = np.argmax(array_2d[array])\n",
    "                    array_2d[array] = np.zeros(9)\n",
    "                    array_2d[array][k] = 1\n",
    "                return array_2d\n",
    "        elif(base_classifier == \"Logistic\"):\n",
    "            if voting == \"hard\":\n",
    "                array_2d = np.zeros((229, 9))\n",
    "                for idx in range(len(predictions[0])):\n",
    "                    uniq, cnt = np.unique(predictions[:, idx], return_counts=True)\n",
    "                    k = np.argmax(cnt)\n",
    "                    array_2d[idx][uniq[k]] = 1\n",
    "                return array_2d\n",
    "\n",
    "            elif voting == \"soft\":\n",
    "                array_2d = np.zeros((229,9))\n",
    "                y_test_copy = y_test.copy()\n",
    "                confidences = [accuracy_score(np.argmax(y_test,axis=1), model.predict(X_test)) for model in ensemble_models]\n",
    "                print(confidences)\n",
    "                confidences = confidences / sum(confidences)\n",
    "                for idx in range(len(predictions[0])):\n",
    "                    uniq = np.zeros(9)\n",
    "                    fin = 0\n",
    "                    wei = 0\n",
    "                    for i in range(len(confidences)):\n",
    "                        uniq[predictions[i][idx]] += confidences[i]\n",
    "                        if(wei < uniq[predictions[i][idx]]):\n",
    "                            wei = uniq[predictions[i][idx]]\n",
    "                            fin = predictions[i][idx]\n",
    "                    array_2d[idx][fin] = 1\n",
    "                return array_2d\n",
    "        elif(base_classifier == \"MLP\"):\n",
    "            if voting == \"hard\":\n",
    "                array_2d = np.sum(predictions, axis=0)\n",
    "                for array in range(len(array_2d)):\n",
    "                    k = np.argmax(array_2d[array])\n",
    "                    array_2d[array] = np.zeros(9)\n",
    "                    array_2d[array][k] = 1\n",
    "                return array_2d\n",
    "\n",
    "            elif voting == \"soft\":\n",
    "                confidences = [accuracy_score(y_test, model.predict(X_test)) for model in ensemble_models]\n",
    "                confidences = confidences / sum(confidences)\n",
    "                print(confidences)\n",
    "                for i in range(len(confidences)):\n",
    "                    predictions[i] = predictions[i] * confidences[i]\n",
    "                array_2d = np.sum(predictions, axis=0)\n",
    "                for array in range(len(array_2d)):\n",
    "                    k = np.argmax(array_2d[array])\n",
    "                    array_2d[array] = np.zeros(9)\n",
    "                    array_2d[array][k] = 1\n",
    "                return array_2d\n",
    "\n",
    "    return ensemble_predict\n",
    "\n",
    "# Example usage for bagging ensemble with Decision Tree classifier and soft voting\n",
    "bagged_classifier_decision_tree_soft = bagging_ensemble_classification(base_classifier=\"Logistic\", n_estimators=10, sample_fraction=0.25, with_replacement=True, voting=\"soft\")\n",
    "\n",
    "# Make predictions using the bagged ensemble\n",
    "y_pred = bagged_classifier_decision_tree_soft(X_test)\n",
    "y_pred = np.array(y_pred)\n",
    "print(y_test.shape)\n",
    "print(y_pred.shape)\n",
    "for i in range(len(y_pred)):\n",
    "    print(y_pred[i])\n",
    "    print(y_test[i])\n",
    "print(y_pred[1])\n",
    "y_test = np.array(y_test)\n",
    "y_pred = np.array(y_pred)\n",
    "# print(\"Pred :\", y_pred)\n",
    "# print(\"Test :\", y_test)\n",
    "# Evaluate the performance, e.g., by calculating the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
