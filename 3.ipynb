{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    def __init__(self):\n",
    "        self.model = DecisionTreeClassifier()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "class MyLogisticClassifier:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.01, num_iterations=1000):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Gradient Descent\n",
    "        for _ in range(num_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            dw = (1/num_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1/num_samples) * np.sum(y_predicted - y)\n",
    "            self.weights -= learning_rate * dw\n",
    "            self.bias -= learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        return y_predicted_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeRegressor:\n",
    "    def __init__(self):\n",
    "        self.model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "class MyLinearRegressor:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "        # print(X.T)\n",
    "        # Closed-form solution for linear regression\n",
    "        X_transpose = X.T\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        # print(y.shape)\n",
    "        # print(X.shape)\n",
    "        # print(X_transpose)\n",
    "        self.weights = np.dot(np.linalg.inv(np.dot(X_transpose, X)), np.dot(X_transpose, y))\n",
    "        # print(self.weights)\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        return linear_model\n",
    "    \n",
    "class MyMLPRegressor(object):\n",
    "    def __init__(self, learning_rate=0.001,epochs = 5000, inputLayerSize: int = 13, hiddenLayerSize = [13,13,13,13,13,13,13,13,13] , outputLayerSize: int = 1):\n",
    "        self.inputLayerSize = inputLayerSize\n",
    "        self.outputLayerSize = outputLayerSize\n",
    "        self.hiddenLayerSize = hiddenLayerSize\n",
    "        self.activation = self.sigmoid\n",
    "        self.activationPrime = self.sigmoidPrime\n",
    "        self.learning_rate = learning_rate\n",
    "        # Weights (parameters)\n",
    "        self.num_layers = len(hiddenLayerSize)+1\n",
    "        self.weights = []\n",
    "        self.weights.append(np.random.randn(self.inputLayerSize, self.hiddenLayerSize[0]))\n",
    "        for i in range(1,self.num_layers-1):\n",
    "            self.weights.append(np.random.randn(self.hiddenLayerSize[i-1], self.hiddenLayerSize[i]))\n",
    "        self.weights.append(np.random.randn(self.hiddenLayerSize[-1], self.outputLayerSize))\n",
    "        self.optimizer = 'MiniBatch'\n",
    "        self.max_iterations = epochs\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z = [None] * (len(self.weights))\n",
    "        self.a = [None] * (len(self.weights) - 1)\n",
    "\n",
    "        self.z[0] = np.dot(X, self.weights[0])\n",
    "        for i in range(0, len(self.a)):\n",
    "            self.a[i] = self.activation(self.z[i])\n",
    "            self.z[i + 1] = np.dot(self.a[i], self.weights[i + 1])\n",
    "        yHat = self.z[-1]  # Linear activation for regression\n",
    "        return yHat\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)  # ReLU activation\n",
    "\n",
    "    def tanh(self, z):\n",
    "        return np.tanh(z)  # Tanh activation\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoidPrime(self, z):\n",
    "        return np.exp(-z) / ((1 + np.exp(-z))**2)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z))  # Subtracting max(z) for numerical stability\n",
    "        return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def costFunction(self, X, y):\n",
    "        self.yHat = self.forward(X)\n",
    "        J = np.sum((y - self.yHat) ** 2) / (2 * len(X))  # Mean Squared Error\n",
    "        return J\n",
    "\n",
    "    def costFunctionPrime(self, X, y):\n",
    "        self.yHat = self.forward(X)\n",
    "        deltas = [None] * (len(self.weights))\n",
    "        deltas[-1] = -(y - self.yHat)\n",
    "        dJdWs = [None] * (len(self.weights))\n",
    "        for i in range(len(deltas) - 2, -1, -1):\n",
    "            dJdWs[i + 1] = np.dot(self.a[i].T, deltas[i + 1])\n",
    "            deltas[i] = np.dot(deltas[i + 1], self.weights[i + 1].T) * self.activationPrime(self.z[i])\n",
    "        dJdWs[0] = np.dot(X.T, deltas[0])\n",
    "        return dJdWs\n",
    "\n",
    "    def reluPrime(self, z):\n",
    "        return np.where(z > 0, 1, 0)  # Derivative of ReLU\n",
    "\n",
    "    def tanhPrime(self, z):\n",
    "        return 1 - np.tanh(z)**2  # Derivative of Tanh\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        dJdWs = self.costFunctionPrime(X, y)\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * dJdWs[i]\n",
    "        # self.W1 -= learning_rate * dJdW1\n",
    "        # self.W2 -= learning_rate * dJdW2\n",
    "    def fit(self, X, y, batch_size = 32, learning_rate=0.1, max_iterations=10000,wand = 0):\n",
    "        if(self.optimizer == 'SGD'):\n",
    "            return self.train_sgd(X=X, y=y,wand=wand)\n",
    "        elif(self.optimizer == 'Batch'):\n",
    "            return self.train_batch(X=X, y=y,wand=wand)\n",
    "        elif(self.optimizer == 'MiniBatch'):\n",
    "            return self.train_mini_batch(X=X, y=y,wand=wand)\n",
    "    def train_sgd(self, X, y, learning_rate=0.1, max_iterations=10000, wand = 0):\n",
    "        if(wand == 1):\n",
    "            wandb.init(project=\"Reporting loss on Housing Dataset in MLP Regression\")\n",
    "        for i in range(self.max_iterations):\n",
    "            idx = np.random.randint(len(X))\n",
    "            self.backward(X[idx:idx+1], y[idx:idx+1], self.learning_rate)\n",
    "            if i % 100 == 0:\n",
    "                loss = self.costFunction(X, y)\n",
    "                print(\"Iteration %d: loss = %f\" % (i, loss))\n",
    "                if(wand == 1):\n",
    "                    wandb.log({\"Loss\": loss, \"Epoch\": i})\n",
    "        loss = self.costFunction(X, y)\n",
    "        return loss\n",
    "        print(\"Training completed.\")\n",
    "    def train_batch(self, X, y, learning_rate=0.0001, max_iterations=10000, wand = 0):\n",
    "        if(wand == 1):\n",
    "            wandb.init(project=\"Reporting loss on Housing Dataset in MLP Regression\")\n",
    "        for i in range(self.max_iterations):\n",
    "            self.backward(X, y, self.learning_rate)\n",
    "            if i % 1 == 0:\n",
    "                loss = self.costFunction(X, y)\n",
    "                # print(\"Iteration %d: loss = %f\" % (i, loss))\n",
    "                if(wand == 1):\n",
    "                    wandb.log({\"Loss\": loss, \"Epoch\": i})\n",
    "        loss = self.costFunction(X, y)\n",
    "        return loss\n",
    "        print(\"Training completed.\")\n",
    "    def train_mini_batch(self, X, y, batch_size=32, learning_rate=0.01, max_iterations=5000, wand = 0):\n",
    "        if(wand == 1):\n",
    "            wandb.init(project=\"Reporting loss on Housing Dataset in MLP Regression\")\n",
    "        for i in range(self.max_iterations):\n",
    "            # Randomly shuffle the data and split into mini-batches\n",
    "            indices = np.arange(len(X))\n",
    "            np.random.shuffle(indices)\n",
    "            for j in range(0, len(X), batch_size):\n",
    "                batch_indices = indices[j:j+batch_size]\n",
    "                self.backward(X[batch_indices], y[batch_indices], self.learning_rate)\n",
    "            if i % 1 == 0:\n",
    "                loss = self.costFunction(X, y)\n",
    "                # print(\"Iteration %d: loss = %f\" % (i, loss))\n",
    "                if(wand == 1):\n",
    "                    wandb.log({\"Loss\": loss, \"Epoch\": i})\n",
    "        loss = self.costFunction(X, y)\n",
    "        return loss\n",
    "        print(\"Training completed.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        yHat = self.forward(X)\n",
    "        return yHat\n",
    "    def set_learning_rate(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def set_activation_function(self, activation_function):\n",
    "        if(activation_function == 'sigmoid'):\n",
    "            self.activation = self.sigmoid\n",
    "            self.activationPrime = self.sigmoidPrime\n",
    "        if(activation_function == 'relu'):\n",
    "            self.activation = self.relu\n",
    "            self.activationPrime = self.reluPrime\n",
    "        if(activation_function == 'tanh'):\n",
    "            self.activation = self.tanh\n",
    "            self.activationPrime = self.tanhPrime\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def set_hidden_layers(self, hidden_layer_sizes):\n",
    "        self.hiddenLayerSize = hidden_layer_sizes\n",
    "        self.num_layers = len(self.hiddenLayerSize)+1\n",
    "        self.weights = []\n",
    "        self.weights.append(np.random.randn(self.inputLayerSize, self.hiddenLayerSize[0]))\n",
    "        for i in range(1,self.num_layers-1):\n",
    "            self.weights.append(np.random.randn(self.hiddenLayerSize[i-1], self.hiddenLayerSize[i]))\n",
    "        self.weights.append(np.random.randn(self.hiddenLayerSize[-1], self.outputLayerSize))\n",
    "        # self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        # self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "        # self.weights, self.biases = self.initialize_weights()\n",
    "    def set_epochs(self, epcohs):\n",
    "        self.max_iterations = epochs\n",
    "        print(self.max_iterations)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('HousingData.csv')\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "data = data.fillna(data.mean())\n",
    "X = data.drop(columns=['MEDV'])\n",
    "y = data['MEDV']\n",
    "# X = data.iloc[1:, :-1].reset_index(drop=True)\n",
    "# y = data.iloc[1:, -1].reset_index(drop=True)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.68762759e-01 0.00000000e+00 6.42962963e-01 ... 8.08510638e-01\n",
      "  8.80427656e-01 6.39624724e-01]\n",
      " [6.95009416e-03 0.00000000e+00 2.74074074e-01 ... 8.93617021e-01\n",
      "  9.96772404e-01 1.85982340e-01]\n",
      " [2.87746689e-04 1.12119342e-01 1.97037037e-01 ... 4.57446809e-01\n",
      "  9.12627969e-01 1.68322296e-01]\n",
      " ...\n",
      " [6.68786251e-05 8.00000000e-01 4.70370370e-02 ... 4.68085106e-01\n",
      "  9.84971506e-01 1.17549669e-01]\n",
      " [1.25342233e-01 0.00000000e+00 6.42962963e-01 ... 8.08510638e-01\n",
      "  2.76186394e-01 5.94370861e-01]\n",
      " [2.46945108e-03 0.00000000e+00 2.89629630e-01 ... 8.82978723e-01\n",
      "  1.77719502e-01 2.45584989e-01]]\n",
      "[12.  19.9 19.4 13.4 18.2 24.6 21.1 24.7  8.7 27.5 20.7 36.2 31.6 11.7\n",
      " 39.8 13.9 21.8 23.7 17.6 24.4  8.8 19.2 25.3 20.4 23.1 37.9 15.6 45.4\n",
      " 15.7 22.6 14.5 18.7 17.8 16.1 20.6 31.6 29.1 15.6 17.5 22.5 19.4 19.3\n",
      "  8.5 20.6 17.  17.1 14.5 50.  14.3 12.6 28.7 21.2 19.3 23.1 19.1 25.\n",
      " 33.4  5.  29.6 18.7 21.7 23.1 22.8 21.  48.8 14.6 16.6 27.1 20.1 19.8\n",
      " 21.  41.3 23.2 20.4 18.5 29.4 36.4 24.4 11.8 13.8 12.3 17.8 33.1 26.7\n",
      " 13.4 14.4 50.  22.  19.9 23.8 17.5 12.7  5.6 31.1 26.2 19.4 16.7 13.8\n",
      " 22.9 15.3 27.5 36.1 22.9 24.5 25.  50.  34.9 31.7 24.1 22.1 14.1 42.8\n",
      " 19.3 32.2 26.4 21.8 21.7  8.3 46.7 43.1 31.5 10.5 16.7 20.  33.3 17.8\n",
      " 50.  20.5 23.2 13.1 19.6 22.8 28.7 30.7 22.9 21.9 23.9 32.7 24.3 21.5\n",
      " 24.6  8.5 26.4 23.1 15.   8.8 19.3 23.9 24.7 19.8 23.8 13.3 29.  27.1\n",
      " 34.6 13.3 15.6 12.5 14.6 11.  24.8 17.3  8.1 21.4 15.6 23.3 32.  38.7\n",
      " 30.1 20.5 32.5 42.3 24.3 20.6 22.  18.2 15.   6.3 20.1 21.4 28.4 30.1\n",
      " 20.8 23.  14.3 11.7 37.3 17.1 10.4 23.  22.7 20.3 21.7 50.   8.4 18.8\n",
      " 37.2 16.1 16.5 22.2 20.6 13.5 48.3 23.8 22.7 17.4 30.3 36.  41.7 18.3\n",
      " 22.  18.6 44.8 11.9 18.7 16.2 22.   7.2 20.4 13.8 13.  18.4 23.1 21.2\n",
      " 23.1 23.5 50.  26.6 22.2 50.   8.3 23.3 21.7 18.9 18.4 17.4 13.4 12.1\n",
      " 26.6 21.7 28.4 20.5 22.  13.9 11.3 29.9 26.6 10.5 23.2 24.4 46.  21.9\n",
      "  7.5 36.2 44.  17.8 27.5 37.6 14.1 28.1 10.2 19.1 43.8 27.9 25.  16.\n",
      " 16.6 13.2 50.  22.2 32.9 15.2 14.8 13.8 24.3 33.8 22.3 50.   9.5 13.3\n",
      " 22.2 18.1 18.  25.  16.5 23.  20.1 33.  24.8 18.2 13.1 34.9 10.2 19.9\n",
      " 27.9 23.3 35.1 12.8 22.  18.5 25.1 22.5 22.4 28.6 19.5 24.8 24.5 21.4\n",
      " 33.1 22.9 20.7 24.1 50.  24.7 28.7  7.2 37.  20.3 30.1 19.5 23.4 11.5\n",
      " 21.6 14.9 15.2 19.4  8.4 28.  22.6 13.5 14.5 31.  10.9 21.9 22.  19.\n",
      " 21.4 25.  17.5 36.5 20.1 20.4 16.2 23.6  7.4 35.2 50.  19.3 21.2 15.6\n",
      " 33.4 19.1 21.  23.7 18.9 16.8 19.7 17.7 22.6 11.8 34.9 20.6 20.2 32.\n",
      " 22.3 23.3 14.4 31.2 24.  29.6 19.6 21.6 20.  27.  33.2 15.4 30.5  7.2\n",
      " 23.9 16.3 23.9 50.  22.8 15.4 19.2 19.6 22.6 33.2 50.  22.2 14.9 19.8\n",
      " 23.7 19.  20.3 11.9 13.6 29.8 21.7 19.5 21.1 24.5 13.4 18.6]\n"
     ]
    }
   ],
   "source": [
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_std = std_scaler.fit_transform(X_train)\n",
    "X_test_std = std_scaler.transform(X_test)\n",
    "\n",
    "X_train_norm = minmax_scaler.fit_transform(X_train_std)\n",
    "X_test_norm = minmax_scaler.transform(X_test_std)\n",
    "X_train = X_train_norm\n",
    "X_test = X_test_norm\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 11.973497772599874\n"
     ]
    }
   ],
   "source": [
    "def bagging_ensemble_regression(base_estimator, n_estimators, sample_fraction,with_replacement, voting):\n",
    "    ensemble_models = []\n",
    "    \n",
    "    sample_size = int(sample_fraction * len(X_train))\n",
    "    # np.random.seed(42)\n",
    "    for _ in range(n_estimators):\n",
    "        sample_indices = np.random.choice(len(X_train), size=sample_size, replace=with_replacement)\n",
    "        sample_X = X_train[sample_indices]\n",
    "        sample_y = y_train[sample_indices]\n",
    "        sample_y = np.array(sample_y)\n",
    "        if base_estimator == \"DecisionTree\":\n",
    "            base_model = MyDecisionTreeRegressor()\n",
    "            base_model.fit(sample_X, sample_y)\n",
    "        elif base_estimator == \"Linear\":\n",
    "            base_model = MyLinearRegressor()\n",
    "            base_model.fit(sample_X, sample_y)\n",
    "        elif base_estimator == \"MLP\":\n",
    "            base_model = MyMLPRegressor()\n",
    "            \n",
    "            y_train_array = np.array([[x] for x in sample_y])\n",
    "            base_model.fit(sample_X, y_train_array)\n",
    "            \n",
    "\n",
    "        # base_model.fit(sample_X, sample_y)\n",
    "        ensemble_models.append(base_model)\n",
    "\n",
    "    def ensemble_predict(X):\n",
    "        predictions = [model.predict(X) for model in ensemble_models]\n",
    "\n",
    "        if voting == \"hard\":\n",
    "            return np.round(np.mean(predictions, axis=0))\n",
    "        elif voting == \"soft\":\n",
    "            confidences = [1 / (mean_squared_error(y_test, model.predict(X_test)) + 1e-10) for model in ensemble_models]  \n",
    "            weighted_predictions = [prediction * confidence for prediction, confidence in zip(predictions, confidences)]\n",
    "            return np.sum(weighted_predictions, axis=0) / np.sum(confidences)\n",
    "\n",
    "    return ensemble_predict\n",
    "\n",
    "# Example usage for bagging ensemble without replacement with Decision Tree regressor and soft voting\n",
    "bagged_regressor_decision_tree_soft = bagging_ensemble_regression(base_estimator=\"DecisionTree\", n_estimators=10, sample_fraction=0.75, with_replacement=True, voting=\"soft\")\n",
    "\n",
    "# Make predictions using the bagged ensemble\n",
    "y_pred = bagged_regressor_decision_tree_soft(X_test)\n",
    "\n",
    "# Evaluate the performance, e.g., by calculating the Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('WineQT.csv', header=0)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2,random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')  \n",
    "\n",
    "train_data_scaled = scaler.fit_transform(imputer.fit_transform(train_data.drop(columns=['quality'])))\n",
    "train_labels = train_data['quality']\n",
    "\n",
    "test_data_scaled = scaler.transform(imputer.transform(test_data.drop(columns=['quality'])))\n",
    "\n",
    "train_data_scaled_normalized = minmax_scaler.fit_transform(train_data_scaled)\n",
    "test_data_scaled_normalized = minmax_scaler.transform(test_data_scaled)\n",
    "\n",
    "X_train = train_data_scaled\n",
    "y_train = train_labels\n",
    "X_test = test_data_scaled\n",
    "y_test = test_data['quality']\n",
    "y_train = np.array(y_train)\n",
    "# y_train = pd.get_dummies(y_train).astype(int)\n",
    "y_train = np.eye(9)[y_train]\n",
    "y_train = np.array(y_train)\n",
    "# y_test = pd.get_dummies(y_test).astype(int)\n",
    "y_test = np.eye(9)[y_test]\n",
    "y_test = np.array(y_test)\n",
    "# print(X_train)\n",
    "# print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 1., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.]]), array([[0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 1., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 1., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 1., 0.]]), array([[0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 1., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 0., ..., 1., 0., 0.]])]\n",
      "(229, 9)\n",
      "(10, 229, 9)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [229, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntulsa/Sem5/SMAI/Assignment 4/3.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(y_pred\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Evaluate the performance, e.g., by calculating the accuracy\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m accuracy \u001b[39m=\u001b[39m accuracy_score(y_test, y_pred)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntulsa/Sem5/SMAI/Assignment%204/3.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m\"\u001b[39m, accuracy)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    222\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     58\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[1;32m     85\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    407\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    408\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    410\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    412\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [229, 10]"
     ]
    }
   ],
   "source": [
    "def bagging_ensemble_classification(base_classifier, n_estimators, sample_fraction, with_replacement, voting):\n",
    "    ensemble_models = []\n",
    "    \n",
    "    sample_size = int(sample_fraction * len(X_train))\n",
    "    # np.random.seed(42)\n",
    "    for _ in range(n_estimators):\n",
    "        sample_indices = np.random.choice(len(X_train), size=sample_size, replace=with_replacement)\n",
    "        sample_X = X_train[sample_indices]  # Assuming X_train is a DataFrame\n",
    "        sample_y = y_train[sample_indices]  # Assuming y_train is a Series\n",
    "\n",
    "        if base_classifier == \"DecisionTree\":\n",
    "            base_model = MyDecisionTreeClassifier()\n",
    "            base_model.fit(sample_X, sample_y)\n",
    "        elif base_classifier == \"Logistic\":\n",
    "            base_model = MyLogisticClassifier()\n",
    "            base_model.fit(sample_X, sample_y)\n",
    "        elif base_classifier == \"MLP\":\n",
    "            base_model = MyMLPClassifier()\n",
    "            base_model.fit(sample_X, sample_y)\n",
    "            \n",
    "        ensemble_models.append(base_model)\n",
    "\n",
    "    def ensemble_predict(X):\n",
    "        predictions = [model.predict(X) for model in ensemble_models]\n",
    "        print(predictions)\n",
    "        return predictions\n",
    "        if voting == \"hard\":\n",
    "            # For hard voting, use the mode (most common prediction)\n",
    "            return np.round(np.mean(predictions, axis=0))\n",
    "        elif voting == \"soft\":\n",
    "            # For soft voting, use the class probabilities (if available) and average them\n",
    "            class_probs = [model.predict(X) for model in ensemble_models]\n",
    "            return np.mean(class_probs, axis=0)\n",
    "\n",
    "    return ensemble_predict\n",
    "\n",
    "# Example usage for bagging ensemble with Decision Tree classifier and soft voting\n",
    "bagged_classifier_decision_tree_soft = bagging_ensemble_classification(base_classifier=\"DecisionTree\", n_estimators=10, sample_fraction=0.75, with_replacement=True, voting=\"soft\")\n",
    "\n",
    "# Make predictions using the bagged ensemble\n",
    "y_pred = bagged_classifier_decision_tree_soft(X_test)\n",
    "y_pred = np.array(y_pred)\n",
    "print(y_test.shape)\n",
    "print(y_pred.shape)\n",
    "# Evaluate the performance, e.g., by calculating the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
