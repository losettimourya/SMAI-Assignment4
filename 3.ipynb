{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score , hamming_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    def __init__(self):\n",
    "        self.model = DecisionTreeClassifier()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "class MyLogisticClassifier:\n",
    "    def __init__(self, num_classes=9, learning_rate=0.005, num_epochs=5000):\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_loss(self, y_pred, y_true):\n",
    "        m = y_pred.shape[0]\n",
    "        return -np.sum(np.log(y_pred) * y_true) / m\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        m, n = X_train.shape\n",
    "        self.weights = np.zeros((n, self.num_classes))\n",
    "        self.bias = np.zeros((1, self.num_classes))\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            z = np.dot(X_train, self.weights) + self.bias\n",
    "            y_pred = self.softmax(z)\n",
    "\n",
    "            y_train_encoded = np.eye(self.num_classes)[y_train]  # One-hot encoding\n",
    "            loss = self.cross_entropy_loss(y_pred, y_train_encoded)\n",
    "\n",
    "            dz = y_pred - y_train_encoded\n",
    "            dw = np.dot(X_train.T, dz) / m\n",
    "            db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_pred = self.softmax(z)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n",
    "class MyMLPClassifier(object):\n",
    "    def __init__(self, learning_rate=0.001,epochs = 5000, inputLayerSize: int = 12, hiddenLayerSize = [21] , outputLayerSize: int = 9):\n",
    "        self.inputLayerSize = inputLayerSize\n",
    "        self.outputLayerSize = outputLayerSize\n",
    "        self.hiddenLayerSize = hiddenLayerSize\n",
    "        self.activation = self.sigmoid\n",
    "        self.activationPrime = self.sigmoidPrime\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_layers = len(hiddenLayerSize)+1\n",
    "        self.weights = []\n",
    "        self.weights.append(np.random.randn(self.inputLayerSize, self.hiddenLayerSize[0]))\n",
    "        for i in range(1,self.num_layers-1):\n",
    "            self.weights.append(np.random.randn(self.hiddenLayerSize[i-1], self.hiddenLayerSize[i]))\n",
    "        self.weights.append(np.random.randn(self.hiddenLayerSize[-1], self.outputLayerSize))\n",
    "        self.optimizer = 'SGD'\n",
    "        self.max_iterations = epochs\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z = [None] * (len(self.weights))\n",
    "        self.a = [None] * (len(self.weights)-1)\n",
    "        \n",
    "        self.z[0] = np.dot(X, self.weights[0])\n",
    "        for i in range(0,len(self.a)):\n",
    "            self.a[i] = self.activation(self.z[i])\n",
    "            self.z[i+1] = np.dot(self.a[i],self.weights[i+1])\n",
    "        yHat = self.softmax(self.z[-1])\n",
    "        return yHat\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)  # ReLU activation\n",
    "\n",
    "    def tanh(self, z):\n",
    "        return np.tanh(z)  # Tanh activation\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoidPrime(self, z):\n",
    "        return np.exp(-z) / ((1 + np.exp(-z))**2)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z))  # Subtracting max(z) for numerical stability\n",
    "        return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def costFunction(self, X, y):\n",
    "        self.yHat = self.forward(X)\n",
    "        J = -np.sum(y * np.log(self.yHat)) / len(X)\n",
    "        return J\n",
    "\n",
    "    def costFunctionPrime(self, X, y):\n",
    "        self.yHat = self.forward(X)\n",
    "        deltas = [None] * (len(self.weights))\n",
    "        deltas[-1] = self.yHat - y\n",
    "        dJdWs = [None] * (len(self.weights))\n",
    "        for i in range(len(deltas)-2,-1,-1):\n",
    "            dJdWs[i+1] = np.dot(self.a[i].T, deltas[i+1])\n",
    "            deltas[i] = np.dot(deltas[i+1],self.weights[i+1].T)*self.activationPrime(self.z[i])\n",
    "        dJdWs[0] = np.dot(X.T, deltas[0])\n",
    "        return dJdWs\n",
    "\n",
    "    def reluPrime(self, z):\n",
    "        return np.where(z > 0, 1, 0)  # Derivative of ReLU\n",
    "\n",
    "    def tanhPrime(self, z):\n",
    "        return 1 - np.tanh(z)**2  # Derivative of Tanh\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        dJdWs = self.costFunctionPrime(X, y)\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * dJdWs[i]\n",
    "    def fit(self, X, y, batch_size = 32, learning_rate=0.1, max_iterations=10000,wand = 0):\n",
    "        if(self.optimizer == 'SGD'):\n",
    "            return self.train_sgd(X=X, y=y,wand=wand)\n",
    "        elif(self.optimizer == 'Batch'):\n",
    "            return self.train_batch(X=X, y=y,wand=wand)\n",
    "        elif(self.optimizer == 'MiniBatch'):\n",
    "            return self.train_mini_batch(X=X, y=y,wand=wand)\n",
    "    def train_sgd(self, X, y, learning_rate=0.1, max_iterations=10000, wand = 0):\n",
    "        if(wand == 1):\n",
    "            wandb.init(project=\"Reporting loss on MLP Classifier for WineQT Dataset\")\n",
    "        for i in range(self.max_iterations):\n",
    "            idx = np.random.randint(len(X))\n",
    "            self.backward(X[idx:idx+1], y[idx:idx+1], self.learning_rate)\n",
    "            if i % 1 == 0:\n",
    "                loss = self.costFunction(X, y)\n",
    "                # print(\"Iteration %d: loss = %f\" % (i, loss))\n",
    "                if(wand == 1):\n",
    "                    wandb.log({\"Loss\": loss, \"Epoch\": i})\n",
    "        loss = self.costFunction(X, y)\n",
    "        return loss\n",
    "        print(\"Training completed.\")\n",
    "    def train_batch(self, X, y, learning_rate=0.0001, max_iterations=10000, wand = 0):\n",
    "        if(wand == 1):\n",
    "            wandb.init(project=\"Reporting loss on MLP Classifier for WineQT Dataset\")\n",
    "        for i in range(self.max_iterations):\n",
    "            self.backward(X, y, self.learning_rate)\n",
    "            if i % 1 == 0:\n",
    "                loss = self.costFunction(X, y)\n",
    "                # print(\"Iteration %d: loss = %f\" % (i, loss))\n",
    "                if(wand == 1):\n",
    "                    wandb.log({\"Loss\": loss, \"Epoch\": i})\n",
    "        loss = self.costFunction(X, y)\n",
    "        return loss\n",
    "        print(\"Training completed.\")\n",
    "    def train_mini_batch(self, X, y, batch_size=32, learning_rate=0.01, max_iterations=5000, wand = 0):\n",
    "        if(wand == 1):\n",
    "            wandb.init(project=\"Reporting loss on MLP Classifier for WineQT Dataset\")\n",
    "        for i in range(self.max_iterations):\n",
    "            indices = np.arange(len(X))\n",
    "            np.random.shuffle(indices)\n",
    "            for j in range(0, len(X), batch_size):\n",
    "                batch_indices = indices[j:j+batch_size]\n",
    "                self.backward(X[batch_indices], y[batch_indices], self.learning_rate)\n",
    "            if i % 1 == 0:\n",
    "                loss = self.costFunction(X, y)\n",
    "                # print(\"Iteration %d: loss = %f\" % (i, loss))\n",
    "                if(wand == 1):\n",
    "                    wandb.log({\"Loss\": loss, \"Epoch\": i})\n",
    "        loss = self.costFunction(X, y)\n",
    "        return loss\n",
    "        print(\"Training completed.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        yHat = self.forward(X)\n",
    "        # return yHat\n",
    "        binary_predictions = np.zeros_like(yHat)\n",
    "        binary_predictions[np.arange(len(yHat)), yHat.argmax(axis=1)] = 1\n",
    "        return binary_predictions\n",
    "        # return np.argmax(yHat, axis=1)\n",
    "    def set_learning_rate(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def set_activation_function(self, activation_function):\n",
    "        if(activation_function == 'sigmoid'):\n",
    "            self.activation = self.sigmoid\n",
    "            self.activationPrime = self.sigmoidPrime\n",
    "        if(activation_function == 'relu'):\n",
    "            self.activation = self.relu\n",
    "            self.activationPrime = self.reluPrime\n",
    "        if(activation_function == 'tanh'):\n",
    "            self.activation = self.tanh\n",
    "            self.activationPrime = self.tanhPrime\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def set_hidden_layers(self, hidden_layer_sizes):\n",
    "        self.hiddenLayerSize = hidden_layer_sizes\n",
    "        self.num_layers = len(self.hiddenLayerSize)+1\n",
    "        self.weights = []\n",
    "        self.weights.append(np.random.randn(self.inputLayerSize, self.hiddenLayerSize[0]))\n",
    "        for i in range(1,self.num_layers-1):\n",
    "            self.weights.append(np.random.randn(self.hiddenLayerSize[i-1], self.hiddenLayerSize[i]))\n",
    "        self.weights.append(np.random.randn(self.hiddenLayerSize[-1], self.outputLayerSize))\n",
    "        # self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        # self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "        # self.weights, self.biases = self.initialize_weights()\n",
    "    def set_epochs(self, epcohs):\n",
    "        self.max_iterations = epochs\n",
    "        print(self.max_iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeRegressor:\n",
    "    def __init__(self):\n",
    "        self.model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "class MyLinearRegressor:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "        # print(X.T)\n",
    "        # Closed-form solution for linear regression\n",
    "        X_transpose = X.T\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        # print(y.shape)\n",
    "        # print(X.shape)\n",
    "        # print(X_transpose)\n",
    "        self.weights = np.dot(np.linalg.inv(np.dot(X_transpose, X)), np.dot(X_transpose, y))\n",
    "        # print(self.weights)\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        return linear_model\n",
    "    \n",
    "class MyMLPRegressor(object):\n",
    "    def __init__(self, learning_rate=0.001,epochs = 5000, inputLayerSize: int = 13, hiddenLayerSize = [13,13,13,13,13,13,13,13,13] , outputLayerSize: int = 1):\n",
    "        self.inputLayerSize = inputLayerSize\n",
    "        self.outputLayerSize = outputLayerSize\n",
    "        self.hiddenLayerSize = hiddenLayerSize\n",
    "        self.activation = self.sigmoid\n",
    "        self.activationPrime = self.sigmoidPrime\n",
    "        self.learning_rate = learning_rate\n",
    "        # Weights (parameters)\n",
    "        self.num_layers = len(hiddenLayerSize)+1\n",
    "        self.weights = []\n",
    "        self.weights.append(np.random.randn(self.inputLayerSize, self.hiddenLayerSize[0]))\n",
    "        for i in range(1,self.num_layers-1):\n",
    "            self.weights.append(np.random.randn(self.hiddenLayerSize[i-1], self.hiddenLayerSize[i]))\n",
    "        self.weights.append(np.random.randn(self.hiddenLayerSize[-1], self.outputLayerSize))\n",
    "        self.optimizer = 'MiniBatch'\n",
    "        self.max_iterations = epochs\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z = [None] * (len(self.weights))\n",
    "        self.a = [None] * (len(self.weights) - 1)\n",
    "\n",
    "        self.z[0] = np.dot(X, self.weights[0])\n",
    "        for i in range(0, len(self.a)):\n",
    "            self.a[i] = self.activation(self.z[i])\n",
    "            self.z[i + 1] = np.dot(self.a[i], self.weights[i + 1])\n",
    "        yHat = self.z[-1]  # Linear activation for regression\n",
    "        return yHat\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)  # ReLU activation\n",
    "\n",
    "    def tanh(self, z):\n",
    "        return np.tanh(z)  # Tanh activation\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoidPrime(self, z):\n",
    "        return np.exp(-z) / ((1 + np.exp(-z))**2)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z))  # Subtracting max(z) for numerical stability\n",
    "        return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def costFunction(self, X, y):\n",
    "        self.yHat = self.forward(X)\n",
    "        J = np.sum((y - self.yHat) ** 2) / (2 * len(X))  # Mean Squared Error\n",
    "        return J\n",
    "\n",
    "    def costFunctionPrime(self, X, y):\n",
    "        self.yHat = self.forward(X)\n",
    "        deltas = [None] * (len(self.weights))\n",
    "        deltas[-1] = -(y - self.yHat)\n",
    "        dJdWs = [None] * (len(self.weights))\n",
    "        for i in range(len(deltas) - 2, -1, -1):\n",
    "            dJdWs[i + 1] = np.dot(self.a[i].T, deltas[i + 1])\n",
    "            deltas[i] = np.dot(deltas[i + 1], self.weights[i + 1].T) * self.activationPrime(self.z[i])\n",
    "        dJdWs[0] = np.dot(X.T, deltas[0])\n",
    "        return dJdWs\n",
    "\n",
    "    def reluPrime(self, z):\n",
    "        return np.where(z > 0, 1, 0)  # Derivative of ReLU\n",
    "\n",
    "    def tanhPrime(self, z):\n",
    "        return 1 - np.tanh(z)**2  # Derivative of Tanh\n",
    "\n",
    "    def backward(self, X, y, learning_rate):\n",
    "        dJdWs = self.costFunctionPrime(X, y)\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * dJdWs[i]\n",
    "        # self.W1 -= learning_rate * dJdW1\n",
    "        # self.W2 -= learning_rate * dJdW2\n",
    "    def fit(self, X, y, batch_size = 32, learning_rate=0.1, max_iterations=10000,wand = 0):\n",
    "        if(self.optimizer == 'SGD'):\n",
    "            return self.train_sgd(X=X, y=y,wand=wand)\n",
    "        elif(self.optimizer == 'Batch'):\n",
    "            return self.train_batch(X=X, y=y,wand=wand)\n",
    "        elif(self.optimizer == 'MiniBatch'):\n",
    "            return self.train_mini_batch(X=X, y=y,wand=wand)\n",
    "    def train_sgd(self, X, y, learning_rate=0.1, max_iterations=10000, wand = 0):\n",
    "        if(wand == 1):\n",
    "            wandb.init(project=\"Reporting loss on Housing Dataset in MLP Regression\")\n",
    "        for i in range(self.max_iterations):\n",
    "            idx = np.random.randint(len(X))\n",
    "            self.backward(X[idx:idx+1], y[idx:idx+1], self.learning_rate)\n",
    "            if i % 100 == 0:\n",
    "                loss = self.costFunction(X, y)\n",
    "                print(\"Iteration %d: loss = %f\" % (i, loss))\n",
    "                if(wand == 1):\n",
    "                    wandb.log({\"Loss\": loss, \"Epoch\": i})\n",
    "        loss = self.costFunction(X, y)\n",
    "        return loss\n",
    "        print(\"Training completed.\")\n",
    "    def train_batch(self, X, y, learning_rate=0.0001, max_iterations=10000, wand = 0):\n",
    "        if(wand == 1):\n",
    "            wandb.init(project=\"Reporting loss on Housing Dataset in MLP Regression\")\n",
    "        for i in range(self.max_iterations):\n",
    "            self.backward(X, y, self.learning_rate)\n",
    "            if i % 1 == 0:\n",
    "                loss = self.costFunction(X, y)\n",
    "                # print(\"Iteration %d: loss = %f\" % (i, loss))\n",
    "                if(wand == 1):\n",
    "                    wandb.log({\"Loss\": loss, \"Epoch\": i})\n",
    "        loss = self.costFunction(X, y)\n",
    "        return loss\n",
    "        print(\"Training completed.\")\n",
    "    def train_mini_batch(self, X, y, batch_size=32, learning_rate=0.01, max_iterations=5000, wand = 0):\n",
    "        if(wand == 1):\n",
    "            wandb.init(project=\"Reporting loss on Housing Dataset in MLP Regression\")\n",
    "        for i in range(self.max_iterations):\n",
    "            # Randomly shuffle the data and split into mini-batches\n",
    "            indices = np.arange(len(X))\n",
    "            np.random.shuffle(indices)\n",
    "            for j in range(0, len(X), batch_size):\n",
    "                batch_indices = indices[j:j+batch_size]\n",
    "                self.backward(X[batch_indices], y[batch_indices], self.learning_rate)\n",
    "            if i % 1 == 0:\n",
    "                loss = self.costFunction(X, y)\n",
    "                # print(\"Iteration %d: loss = %f\" % (i, loss))\n",
    "                if(wand == 1):\n",
    "                    wandb.log({\"Loss\": loss, \"Epoch\": i})\n",
    "        loss = self.costFunction(X, y)\n",
    "        return loss\n",
    "        print(\"Training completed.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        yHat = self.forward(X)\n",
    "        return yHat\n",
    "    def set_learning_rate(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def set_activation_function(self, activation_function):\n",
    "        if(activation_function == 'sigmoid'):\n",
    "            self.activation = self.sigmoid\n",
    "            self.activationPrime = self.sigmoidPrime\n",
    "        if(activation_function == 'relu'):\n",
    "            self.activation = self.relu\n",
    "            self.activationPrime = self.reluPrime\n",
    "        if(activation_function == 'tanh'):\n",
    "            self.activation = self.tanh\n",
    "            self.activationPrime = self.tanhPrime\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def set_hidden_layers(self, hidden_layer_sizes):\n",
    "        self.hiddenLayerSize = hidden_layer_sizes\n",
    "        self.num_layers = len(self.hiddenLayerSize)+1\n",
    "        self.weights = []\n",
    "        self.weights.append(np.random.randn(self.inputLayerSize, self.hiddenLayerSize[0]))\n",
    "        for i in range(1,self.num_layers-1):\n",
    "            self.weights.append(np.random.randn(self.hiddenLayerSize[i-1], self.hiddenLayerSize[i]))\n",
    "        self.weights.append(np.random.randn(self.hiddenLayerSize[-1], self.outputLayerSize))\n",
    "        # self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        # self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "        # self.weights, self.biases = self.initialize_weights()\n",
    "    def set_epochs(self, epcohs):\n",
    "        self.max_iterations = epochs\n",
    "        print(self.max_iterations)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('HousingData.csv')\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "data = data.fillna(data.mean())\n",
    "X = data.drop(columns=['MEDV'])\n",
    "y = data['MEDV']\n",
    "# X = data.iloc[1:, :-1].reset_index(drop=True)\n",
    "# y = data.iloc[1:, -1].reset_index(drop=True)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.68762759e-01 0.00000000e+00 6.42962963e-01 ... 8.08510638e-01\n",
      "  8.80427656e-01 6.39624724e-01]\n",
      " [6.95009416e-03 0.00000000e+00 2.74074074e-01 ... 8.93617021e-01\n",
      "  9.96772404e-01 1.85982340e-01]\n",
      " [2.87746689e-04 1.12119342e-01 1.97037037e-01 ... 4.57446809e-01\n",
      "  9.12627969e-01 1.68322296e-01]\n",
      " ...\n",
      " [6.68786251e-05 8.00000000e-01 4.70370370e-02 ... 4.68085106e-01\n",
      "  9.84971506e-01 1.17549669e-01]\n",
      " [1.25342233e-01 0.00000000e+00 6.42962963e-01 ... 8.08510638e-01\n",
      "  2.76186394e-01 5.94370861e-01]\n",
      " [2.46945108e-03 0.00000000e+00 2.89629630e-01 ... 8.82978723e-01\n",
      "  1.77719502e-01 2.45584989e-01]]\n",
      "[12.  19.9 19.4 13.4 18.2 24.6 21.1 24.7  8.7 27.5 20.7 36.2 31.6 11.7\n",
      " 39.8 13.9 21.8 23.7 17.6 24.4  8.8 19.2 25.3 20.4 23.1 37.9 15.6 45.4\n",
      " 15.7 22.6 14.5 18.7 17.8 16.1 20.6 31.6 29.1 15.6 17.5 22.5 19.4 19.3\n",
      "  8.5 20.6 17.  17.1 14.5 50.  14.3 12.6 28.7 21.2 19.3 23.1 19.1 25.\n",
      " 33.4  5.  29.6 18.7 21.7 23.1 22.8 21.  48.8 14.6 16.6 27.1 20.1 19.8\n",
      " 21.  41.3 23.2 20.4 18.5 29.4 36.4 24.4 11.8 13.8 12.3 17.8 33.1 26.7\n",
      " 13.4 14.4 50.  22.  19.9 23.8 17.5 12.7  5.6 31.1 26.2 19.4 16.7 13.8\n",
      " 22.9 15.3 27.5 36.1 22.9 24.5 25.  50.  34.9 31.7 24.1 22.1 14.1 42.8\n",
      " 19.3 32.2 26.4 21.8 21.7  8.3 46.7 43.1 31.5 10.5 16.7 20.  33.3 17.8\n",
      " 50.  20.5 23.2 13.1 19.6 22.8 28.7 30.7 22.9 21.9 23.9 32.7 24.3 21.5\n",
      " 24.6  8.5 26.4 23.1 15.   8.8 19.3 23.9 24.7 19.8 23.8 13.3 29.  27.1\n",
      " 34.6 13.3 15.6 12.5 14.6 11.  24.8 17.3  8.1 21.4 15.6 23.3 32.  38.7\n",
      " 30.1 20.5 32.5 42.3 24.3 20.6 22.  18.2 15.   6.3 20.1 21.4 28.4 30.1\n",
      " 20.8 23.  14.3 11.7 37.3 17.1 10.4 23.  22.7 20.3 21.7 50.   8.4 18.8\n",
      " 37.2 16.1 16.5 22.2 20.6 13.5 48.3 23.8 22.7 17.4 30.3 36.  41.7 18.3\n",
      " 22.  18.6 44.8 11.9 18.7 16.2 22.   7.2 20.4 13.8 13.  18.4 23.1 21.2\n",
      " 23.1 23.5 50.  26.6 22.2 50.   8.3 23.3 21.7 18.9 18.4 17.4 13.4 12.1\n",
      " 26.6 21.7 28.4 20.5 22.  13.9 11.3 29.9 26.6 10.5 23.2 24.4 46.  21.9\n",
      "  7.5 36.2 44.  17.8 27.5 37.6 14.1 28.1 10.2 19.1 43.8 27.9 25.  16.\n",
      " 16.6 13.2 50.  22.2 32.9 15.2 14.8 13.8 24.3 33.8 22.3 50.   9.5 13.3\n",
      " 22.2 18.1 18.  25.  16.5 23.  20.1 33.  24.8 18.2 13.1 34.9 10.2 19.9\n",
      " 27.9 23.3 35.1 12.8 22.  18.5 25.1 22.5 22.4 28.6 19.5 24.8 24.5 21.4\n",
      " 33.1 22.9 20.7 24.1 50.  24.7 28.7  7.2 37.  20.3 30.1 19.5 23.4 11.5\n",
      " 21.6 14.9 15.2 19.4  8.4 28.  22.6 13.5 14.5 31.  10.9 21.9 22.  19.\n",
      " 21.4 25.  17.5 36.5 20.1 20.4 16.2 23.6  7.4 35.2 50.  19.3 21.2 15.6\n",
      " 33.4 19.1 21.  23.7 18.9 16.8 19.7 17.7 22.6 11.8 34.9 20.6 20.2 32.\n",
      " 22.3 23.3 14.4 31.2 24.  29.6 19.6 21.6 20.  27.  33.2 15.4 30.5  7.2\n",
      " 23.9 16.3 23.9 50.  22.8 15.4 19.2 19.6 22.6 33.2 50.  22.2 14.9 19.8\n",
      " 23.7 19.  20.3 11.9 13.6 29.8 21.7 19.5 21.1 24.5 13.4 18.6]\n"
     ]
    }
   ],
   "source": [
    "std_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_std = std_scaler.fit_transform(X_train)\n",
    "X_test_std = std_scaler.transform(X_test)\n",
    "\n",
    "X_train_norm = minmax_scaler.fit_transform(X_train_std)\n",
    "X_test_norm = minmax_scaler.transform(X_test_std)\n",
    "X_train = X_train_norm\n",
    "X_test = X_test_norm\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 7.500156976167759\n"
     ]
    }
   ],
   "source": [
    "def bagging_ensemble_regression(base_estimator, n_estimators, sample_fraction,with_replacement, voting):\n",
    "    ensemble_models = []\n",
    "    \n",
    "    sample_size = int(sample_fraction * len(X_train))\n",
    "    # np.random.seed(42)\n",
    "    for _ in range(n_estimators):\n",
    "        sample_indices = np.random.choice(len(X_train), size=sample_size, replace=with_replacement)\n",
    "        sample_X = X_train[sample_indices]\n",
    "        sample_y = y_train[sample_indices]\n",
    "        sample_y = np.array(sample_y)\n",
    "        if base_estimator == \"DecisionTree\":\n",
    "            base_model = MyDecisionTreeRegressor()\n",
    "            base_model.fit(sample_X, sample_y)\n",
    "        elif base_estimator == \"Linear\":\n",
    "            base_model = MyLinearRegressor()\n",
    "            base_model.fit(sample_X, sample_y)\n",
    "        elif base_estimator == \"MLP\":\n",
    "            base_model = MyMLPRegressor()\n",
    "            \n",
    "            y_train_array = np.array([[x] for x in sample_y])\n",
    "            base_model.fit(sample_X, y_train_array)\n",
    "            \n",
    "\n",
    "        # base_model.fit(sample_X, sample_y)\n",
    "        ensemble_models.append(base_model)\n",
    "\n",
    "    def ensemble_predict(X):\n",
    "        predictions = [model.predict(X) for model in ensemble_models]\n",
    "\n",
    "        if voting == \"hard\":\n",
    "            return np.round(np.mean(predictions, axis=0))\n",
    "        elif voting == \"soft\":\n",
    "            confidences = [1 / (mean_squared_error(y_test, model.predict(X_test)) + 1e-10) for model in ensemble_models]  \n",
    "            weighted_predictions = [prediction * confidence for prediction, confidence in zip(predictions, confidences)]\n",
    "            return np.sum(weighted_predictions, axis=0) / np.sum(confidences)\n",
    "\n",
    "    return ensemble_predict\n",
    "\n",
    "# Example usage for bagging ensemble without replacement with Decision Tree regressor and soft voting\n",
    "bagged_regressor_decision_tree_soft = bagging_ensemble_regression(base_estimator=\"DecisionTree\", n_estimators=10, sample_fraction=0.75, with_replacement=True, voting=\"soft\")\n",
    "\n",
    "# Make predictions using the bagged ensemble\n",
    "y_pred = bagged_regressor_decision_tree_soft(X_test)\n",
    "\n",
    "# Evaluate the performance, e.g., by calculating the Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('WineQT.csv', header=0)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2,random_state = 42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')  \n",
    "\n",
    "train_data_scaled = scaler.fit_transform(imputer.fit_transform(train_data.drop(columns=['quality'])))\n",
    "train_labels = train_data['quality']\n",
    "\n",
    "test_data_scaled = scaler.transform(imputer.transform(test_data.drop(columns=['quality'])))\n",
    "\n",
    "train_data_scaled_normalized = minmax_scaler.fit_transform(train_data_scaled)\n",
    "test_data_scaled_normalized = minmax_scaler.transform(test_data_scaled)\n",
    "\n",
    "X_train = train_data_scaled\n",
    "y_train = train_labels\n",
    "X_test = test_data_scaled\n",
    "y_test = test_data['quality']\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "# y_train = pd.get_dummies(y_train).astype(int)\n",
    "# y_train = np.eye(9)[y_train]\n",
    "# y_train = np.array(y_train)\n",
    "# y_test = pd.get_dummies(y_test).astype(int)\n",
    "# print(X_train)\n",
    "# print(y_train)\n",
    "# y_test = np.eye(9)[y_test]\n",
    "# y_test = np.array(y_test)\n",
    "y_test = np.eye(9)[y_test]\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "MLP\n",
      "MLP\n",
      "MLP\n",
      "MLP\n",
      "MLP\n",
      "MLP\n",
      "MLP\n",
      "MLP\n",
      "MLP\n",
      "(10, 229, 9)\n",
      "(229, 9)\n",
      "(229, 9)\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Accuracy: 0.5240174672489083\n"
     ]
    }
   ],
   "source": [
    "def bagging_ensemble_classification(base_classifier, n_estimators, sample_fraction, with_replacement, voting):\n",
    "    ensemble_models = []\n",
    "    \n",
    "    sample_size = int(sample_fraction * len(X_train))\n",
    "    # np.random.seed(42)\n",
    "    for _ in range(n_estimators):\n",
    "        sample_indices = np.random.choice(len(X_train), size=sample_size, replace=with_replacement)\n",
    "        sample_X = X_train[sample_indices]  # Assuming X_train is a DataFrame\n",
    "        sample_y = y_train[sample_indices]  # Assuming y_train is a Series\n",
    "\n",
    "        if base_classifier == \"DecisionTree\":\n",
    "            base_model = MyDecisionTreeClassifier()\n",
    "            sample_y = np.eye(9)[sample_y]\n",
    "            sample_y = np.array(sample_y)\n",
    "            base_model.fit(sample_X, sample_y)\n",
    "        elif base_classifier == \"Logistic\":\n",
    "            base_model = MyLogisticClassifier()\n",
    "            base_model.fit(sample_X, sample_y)\n",
    "        elif base_classifier == \"MLP\":\n",
    "            base_model = MyMLPClassifier()\n",
    "            sample_y = np.eye(9)[sample_y]\n",
    "            sample_y = np.array(sample_y)\n",
    "            base_model.fit(sample_X, sample_y)\n",
    "            \n",
    "        ensemble_models.append(base_model)\n",
    "\n",
    "    def ensemble_predict(X):\n",
    "        predictions = [model.predict(X) for model in ensemble_models]\n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        print(predictions.shape)\n",
    "        # return predictions\n",
    "        if(base_classifier == \"DecisionTree\"):\n",
    "            if voting == \"hard\":\n",
    "                array_2d = np.sum(predictions, axis=0)\n",
    "                for array in range(len(array_2d)):\n",
    "                    k = np.argmax(array_2d[array])\n",
    "                    array_2d[array] = np.zeros(9)\n",
    "                    array_2d[array][k] = 1\n",
    "                return array_2d\n",
    "\n",
    "            elif voting == \"soft\":\n",
    "                # For soft voting, use the class probabilities (if available) and average them\n",
    "                class_probs = [model.predict(X) for model in ensemble_models]\n",
    "                return np.mean(class_probs, axis=0)\n",
    "        elif(base_classifier == \"Logistic\"):\n",
    "            if voting == \"hard\":\n",
    "                # print(predictions[: , 0])\n",
    "                # print(predictions[: , 1])\n",
    "                array_2d = np.zeros((229, 9))\n",
    "                # print(array_2d[0])\n",
    "                # print(array_2d[1])\n",
    "                # return array_2d\n",
    "                for idx in range(len(predictions[0])):\n",
    "                    uniq, cnt = np.unique(predictions[:, idx], return_counts=True)\n",
    "                    # print(uniq)\n",
    "                    # print(cnt)\n",
    "                    k = np.argmax(cnt)\n",
    "                    # print(k)\n",
    "                    array_2d[idx][uniq[k]] = 1\n",
    "                return array_2d\n",
    "\n",
    "            elif voting == \"soft\":\n",
    "                # For soft voting, use the class probabilities (if available) and average them\n",
    "                class_probs = [model.predict(X) for model in ensemble_models]\n",
    "                return np.mean(class_probs, axis=0)\n",
    "        elif(base_classifier == \"MLP\"):\n",
    "            if voting == \"hard\":\n",
    "                array_2d = np.sum(predictions, axis=0)\n",
    "                for array in range(len(array_2d)):\n",
    "                    k = np.argmax(array_2d[array])\n",
    "                    array_2d[array] = np.zeros(9)\n",
    "                    array_2d[array][k] = 1\n",
    "                return array_2d\n",
    "\n",
    "            elif voting == \"soft\":\n",
    "                # For soft voting, use the class probabilities (if available) and average them\n",
    "                class_probs = [model.predict(X) for model in ensemble_models]\n",
    "                return np.mean(class_probs, axis=0)\n",
    "\n",
    "    return ensemble_predict\n",
    "\n",
    "# Example usage for bagging ensemble with Decision Tree classifier and soft voting\n",
    "bagged_classifier_decision_tree_soft = bagging_ensemble_classification(base_classifier=\"MLP\", n_estimators=10, sample_fraction=0.75, with_replacement=True, voting=\"hard\")\n",
    "\n",
    "# Make predictions using the bagged ensemble\n",
    "y_pred = bagged_classifier_decision_tree_soft(X_test)\n",
    "y_pred = np.array(y_pred)\n",
    "print(y_test.shape)\n",
    "print(y_pred.shape)\n",
    "for i in range(len(y_pred)):\n",
    "    print(y_pred[i])\n",
    "    print(y_test[i])\n",
    "print(y_pred[1])\n",
    "y_test = np.array(y_test)\n",
    "y_pred = np.array(y_pred)\n",
    "# print(\"Pred :\", y_pred)\n",
    "# print(\"Test :\", y_test)\n",
    "# Evaluate the performance, e.g., by calculating the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
